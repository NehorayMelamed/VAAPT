














# class UNET_Up_General_V1(nn.Module):
#     def __init__(self,
#                  number_of_lower_level_channels,
#                  number_of_lower_level_channels_after_upsample,
#                  number_of_cross_connection_channels,
#                  number_of_output_channels,
#                  flag_use_final_projection_block=True,
#                  number_of_output_channels_after_projection_block=32,
#                  flag_use_cross_connection=True,
#                  flag_Sequential_or_RDB='sequential',  # 'sequential' / 'rdb'
#                  flag_sequential_dense=False,
#                  flag_sequential_resnet=False,
#                  flag_sequential_concat=False,
#                  stack_residual_scale=1,
#                  kernel_sizes=3, strides=1, dilations=1, groups=1, normalization_function='instance_normalization', activation_function='prelu',
#                  flag_upsample_method='bilinear',
#                  flag_add_unshuffled_input_to_upper_level=False,
#                  initialization_method='xavier',
#                  flag_deformable_convolution=False,
#                  flag_deformable_convolution_version='v1',
#                  flag_deformable_convolution_before_or_after_main_convolution='before',  # 'before' / 'after'
#                  flag_deformable_convolution_modulation=False,
#                  flag_single_cell_block_type='simple',  # 'simple'/ 'standard_residual'/ '131_residual'
#                  flag_super_cell_block_type='131_collapse_residual', # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
#                  flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
#                  flag_deformable_same_on_all_channels=True,
#                  ):
#         super(UNET_Up_General_V1, self).__init__()
#
#         # ##############################################   --  1-K-1 --(*+)---
#         # ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
#         # flag_SuperBlock_SFT = False,
#         # flag_SuperBlock_SFT_use_outside_conditional = False,
#         # flag_SuperBlock_SFT_same_on_all_channels = False,
#         # flag_SuperBlock_SFT_base_convs_mix = 'x',  # 'x', 'y', 'xy'
#         # flag_SuperBlock_SFT_SFT_convs_mix = 'x',  # 'x', 'y', 'xy'
#         # flag_SuperBlock_SFT_add_y_to_output = False,
#         # flag_SuperBlock_SFT_shift = False,
#         # flag_SuperBlock_SFT_scale = False,
#         # ### Deformable Convolution: ###
#         # flag_deformable_convolution = False,
#         # flag_deformable_convolution_version = 'v3',
#         # flag_deformable_convolution_before_or_after_main_convolution = 'before',  # 'before' / 'after'
#         # flag_deformable_convolution_modulation = True,
#         # flag_deformable_kernel_size = 5,
#         # flag_deformable_number_of_deformable_groups = -1,
#         # flag_deformable_number_of_channels_in_group1 = -1,
#         # flag_deformable_same_on_all_channels = True,
#         # ### Deformable SFT: ###
#         # flag_deformable_SFT_use_outside_conditional = False,
#         # flag_deformable_SFT_same_on_all_channels = False,  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
#         # flag_deformable_SFT_base_convs_mix = 'x',
#         # flag_deformable_SFT_SFT_convs_mix = 'x',
#         # flag_deformable_SFT_shift = False,
#         # flag_deformable_SFT_scale = False,
#         # flag_deformable_SFT_add_y_to_output = False,
#         # flag_deformable_for_each_sub_block_or_for_super_block = 'super_block',  # 'super_block' / 'sub_block'
#         # #####################################
#
#         #Take Care Of Variables:
#         if type(number_of_output_channels)!=list and type(number_of_output_channels)!=tuple:
#             number_of_output_channels = [number_of_output_channels]
#         number_of_layers = len(number_of_output_channels);
#         kernel_sizes = to_list_of_certain_size(kernel_sizes, number_of_layers)
#         strides = to_list_of_certain_size(strides, number_of_layers)
#         dilations = to_list_of_certain_size(dilations, number_of_layers)
#         groups = to_list_of_certain_size(groups, number_of_layers)
#
#         self.number_of_cross_connection_channels = number_of_cross_connection_channels;
#         self.flag_use_cross_connection = flag_use_cross_connection;
#         if flag_use_cross_connection==False:
#             number_of_cross_connection_channels = 0;
#
#         #(*). Upsample Method For Low Layer Input:
#         if flag_upsample_method=='deconvolution' or 'transpose' in flag_upsample_method:
#             self.up = nn.ConvTranspose2d(number_of_lower_level_channels, number_of_lower_level_channels_after_upsample, kernel_size=2, stride=2)
#         elif flag_upsample_method=='bilinear':
#             self.up = nn.UpsamplingBilinear2d(scale_factor=2)
#         elif 'pytorch_smart_shuffle' in flag_upsample_method:
#             self.up = nn.PixelShuffle(upscale_factor=2)
#         elif flag_upsample_method == 'my_smart_shuffle':
#             self.up = Pixel_Shuffle_Block(number_of_lower_level_channels,
#                                           number_of_lower_level_channels_after_upsample,
#                                           upscale_factor=2,
#                                           kernel_size=3, stride=1, bias=True, padding_type='zero',
#                                           normalization_function=None,
#                                           activation_function=activation_function)
#         elif flag_upsample_method == 'my_smart_shuffle_stacked':
#             self.up = Pixel_Shuffle_Block_Stacked(number_of_lower_level_channels, number_of_lower_level_channels_after_upsample,
#                                                   upscale_factor=2, number_of_layers = 2,
#                                                   kernel_size=3, stride=1, bias=True, padding_type='zero', normalization_function=None, activation_function=activation_function)
#         elif flag_upsample_method == 'simple_shuffle':
#             self.up = ShufflePixels(2)
#
#
#         ### If the upsampling method is using shuffle then the number of channels goes down by 4: ###
#         self.flag_lower_level_shuffled = ('pytorch_smart_shuffle' in flag_upsample_method or 'simple_shuffle' in flag_upsample_method)
#         if self.flag_lower_level_shuffled:
#             number_of_lower_level_channels_after_upsample = int(number_of_lower_level_channels/4);
#
#
#         #(*). Fusion Strategy (concat -> conv):
#         if flag_Sequential_or_RDB == 'sequential':
#             self.conv_block_on_fused_inputs = Sequential_Conv_Block_General(
#                                               number_of_input_channels=number_of_lower_level_channels_after_upsample + number_of_cross_connection_channels,
#                                                number_of_output_channels=number_of_output_channels,
#                                                kernel_sizes=kernel_sizes,
#                                                strides=strides,
#                                                dilations=dilations,
#                                                groups=groups,
#                                                padding_type='reflect',
#                                                normalization_function=normalization_function,
#                                                activation_function=activation_function,
#                                                mode='CNA',
#                                                initialization_method=initialization_method,
#                                                flag_dense=flag_sequential_dense,
#                                                flag_resnet=flag_sequential_resnet,
#                                                flag_concat=flag_sequential_concat,
#                                                stack_residual_scale=stack_residual_scale,
#                                                flag_deformable_convolution=flag_deformable_convolution,
#                                                flag_deformable_convolution_version=flag_deformable_convolution_version,
#                                                flag_deformable_convolution_before_or_after_main_convolution=flag_deformable_convolution_before_or_after_main_convolution,  # 'before' / 'after'
#                                                flag_deformable_convolution_modulation=flag_deformable_convolution_modulation,
#                                                flag_single_cell_block_type=flag_single_cell_block_type,
#                                                flag_super_cell_block_type=flag_super_cell_block_type,
#                                                flag_deformable_for_each_sub_block_or_for_super_block=flag_deformable_for_each_sub_block_or_for_super_block,
#                                                flag_deformable_same_on_all_channels=flag_deformable_same_on_all_channels)
#         elif flag_Sequential_or_RDB == 'rdb':
#             self.conv_block_on_fused_inputs = RDB(number_of_input_channels=number_of_input_channels_from_upper_layer + number_of_input_channels_from_cross_connection,
#                              number_of_output_channels_for_each_conv_block=number_of_output_channels,
#                              number_of_conv_blocks=number_of_layers,
#                              kernel_size=kernel_sizes, stride=1, bias=True, padding_type='reflect', normalization_function=None, activation_function='leakyrelu', mode='CNA',
#                              final_residual_branch_scale_factor=1/number_of_layers)
#
#         ### Projection Block at the end: ###
#         if flag_use_final_projection_block:
#             self.projection_block = Color_Space_Conversion_Layer(number_of_input_channels=self.conv_block_on_fused_inputs.final_number_of_channels,
#                                                                  number_of_output_channels=number_of_output_channels_after_projection_block)
#             self.conv_block_on_fused_inputs = nn.Sequential(self.conv_block_on_fused_inputs, self.projection_block)
#             self.final_number_of_channels = number_of_output_channels_after_projection_block
#         else:
#             self.final_number_of_channels = self.conv_block_on_fused_inputs.final_number_of_channels
#
#         # if flag_sequential_concat:
#         #     self.final_number_of_channels += number_of_lower_level_channels_after_upsample + number_of_cross_connection_channels
#
#
#     def forward(self, input_cross_connection, input_low_layer, y_cell_conditional=None, y_deformable_conditional=None):
#         #(1). upsample input coming from lower layer:
#         lower_level_upsampled = self.up(input_low_layer)
#
#         if self.flag_use_cross_connection == False:
#             ### No Cross Conection!: ###
#             return self.conv_block_on_fused_inputs(lower_level_upsampled, y_cell_conditional, y_deformable_conditional)
#         else:
#             ### With Cross Connection!: ###
#             # Correct For cross connection and upsample layer output size discrepency:
#             offset = input_cross_connection.size()[2] - lower_level_upsampled.size()[2]  # offset SHOULD BE either 0 or 1 (for instance downsampling from 25 to 12 and the upscaling from 12 to 24)
#             padding = [offset, 0, offset, 0]
#             lower_level_upsampled_padded = F.pad(lower_level_upsampled, padding, 'reflect')
#             # Fuse Information by concatenating and convolving:
#             return self.conv_block_on_fused_inputs(torch.cat([input_cross_connection, lower_level_upsampled_padded], 1), y_cell_conditional, y_deformable_conditional)

















# class UNET_Down_Fusion_General_V1(nn.Module):
#     #TODO: maybe add the possiblity of a "memory" which simply holds the previous output ???
#     def __init__(self,
#                  number_of_input_channels_from_upper_layer,
#                  number_of_input_channels_from_cross_connection,
#                  number_of_output_channels,
#                  flag_use_final_projection_block = False,
#                  number_of_output_channels_after_projection_block = 32,
#                  flag_use_cross_connection=True,
#                  flag_Sequential_or_RDB = 'sequential', #'sequential' / 'rdb'
#                  flag_sequential_dense = False,
#                  flag_sequential_resnet = False,
#                  flag_sequential_concat = True,
#                  stack_residual_scale = 1,
#                  kernel_sizes=3, strides=1, dilations=1, groups=1,
#                  normalization_function='instance_normalization',
#                  activation_function='prelu',
#                  initialization_method = 'dirac',
#                  flag_downsample_strategy='maxpool',  #'maxpool' / 'unshuffle'
#                  flag_add_unshuffled_input_to_lower_level=True,
#                  flag_deformable_convolution=False,
#                  flag_deformable_convolution_version='v1',
#                  flag_deformable_convolution_before_or_after_main_convolution='before',  # 'before' / 'after'
#                  flag_deformable_convolution_modulation=False,
#                  flag_single_cell_block_type='simple',  # 'simple'/ 'standard_residual'/ '131_residual'
#                  flag_super_cell_block_type='131_collapse_residual', # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
#                  flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
#                  flag_deformable_same_on_all_channels=True, ):
#         super(UNET_Down_Fusion_General_V1, self).__init__()
#
#         # ##############################################   --  1-K-1 --(*+)---
#         # ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
#         # flag_SuperBlock_SFT = False,
#         # flag_SuperBlock_SFT_use_outside_conditional = False,
#         # flag_SuperBlock_SFT_same_on_all_channels = False,
#         # flag_SuperBlock_SFT_base_convs_mix = 'x',  # 'x', 'y', 'xy'
#         # flag_SuperBlock_SFT_SFT_convs_mix = 'x',  # 'x', 'y', 'xy'
#         # flag_SuperBlock_SFT_add_y_to_output = False,
#         # flag_SuperBlock_SFT_shift = False,
#         # flag_SuperBlock_SFT_scale = False,  ### Deformable Convolution: ###
#         # flag_deformable_convolution = False,
#         # flag_deformable_convolution_version = 'v3',
#         # flag_deformable_convolution_before_or_after_main_convolution = 'before',  # 'before' / 'after'
#         # flag_deformable_convolution_modulation = True,
#         # flag_deformable_kernel_size = 5,
#         # flag_deformable_number_of_deformable_groups = -1,
#         # flag_deformable_number_of_channels_in_group1 = -1,
#         # flag_deformable_same_on_all_channels = True,  ### Deformable SFT: ###
#         # flag_deformable_SFT_use_outside_conditional = False,
#         # flag_deformable_SFT_same_on_all_channels = False,  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
#         # flag_deformable_SFT_base_convs_mix = 'x',
#         # flag_deformable_SFT_SFT_convs_mix = 'x',
#         # flag_deformable_SFT_shift = False,
#         # flag_deformable_SFT_scale = False,
#         # flag_deformable_SFT_add_y_to_output = False,
#         # flag_deformable_for_each_sub_block_or_for_super_block = 'super_block',  # 'super_block' / 'sub_block'
#         # #####################################
#         # ### Cell and Super-Cell Types: ######
#         # flag_single_cell_block_type = 'simple',  # 'simple'/ 'standard_residual'/ '131_residual'
#         # flag_super_cell_block_type = 'concat',  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
#
#
#
#
#         # Take Care Of Variables:
#         if type(number_of_output_channels)!=list and type(number_of_output_channels)!=tuple:
#             number_of_output_channels = [number_of_output_channels]
#         number_of_layers = len(number_of_output_channels);
#         kernel_sizes = to_list_of_certain_size(kernel_sizes, number_of_layers)
#         strides = to_list_of_certain_size(strides, number_of_layers)
#         dilations = to_list_of_certain_size(dilations, number_of_layers)
#         groups = to_list_of_certain_size(groups, number_of_layers)
#
#         self.flag_add_unshuffled_input_to_lower_level = flag_add_unshuffled_input_to_lower_level
#         self.flag_use_cross_connection = flag_use_cross_connection;
#         if flag_use_cross_connection==False:
#             number_of_input_channels_from_cross_connection = 0;
#
#         #Process Input From Upper Layer:
#         if flag_Sequential_or_RDB == 'sequential':
#             self.conv1 = Sequential_Conv_Block_General(number_of_input_channels=number_of_input_channels_from_upper_layer + number_of_input_channels_from_cross_connection,
#                                                number_of_output_channels=number_of_output_channels,
#                                                kernel_sizes=kernel_sizes,
#                                                strides=strides,
#                                                dilations=dilations,
#                                                groups=groups,
#                                                padding_type='reflect',
#                                                normalization_function=normalization_function,
#                                                activation_function=activation_function,
#                                                mode='CNA',
#                                                initialization_method=initialization_method,
#                                                flag_dense=flag_sequential_dense,
#                                                flag_resnet=flag_sequential_resnet,
#                                                flag_concat=flag_sequential_concat,
#                                                stack_residual_scale = stack_residual_scale,
#                                                flag_deformable_convolution=flag_deformable_convolution,
#                                                flag_deformable_convolution_version=flag_deformable_convolution_version,
#                                                flag_deformable_convolution_before_or_after_main_convolution=flag_deformable_convolution_before_or_after_main_convolution,  # 'before' / 'after'
#                                                flag_deformable_convolution_modulation=flag_deformable_convolution_modulation,
#                                                flag_single_cell_block_type=flag_single_cell_block_type,
#                                                flag_super_cell_block_type=flag_super_cell_block_type,
#                                                flag_deformable_for_each_sub_block_or_for_super_block=flag_deformable_for_each_sub_block_or_for_super_block,
#                                                flag_deformable_same_on_all_channels=flag_deformable_same_on_all_channels
#                                                        )
#         elif flag_Sequential_or_RDB == 'rdb':
#             self.conv1 = RDB(number_of_input_channels=number_of_input_channels_from_upper_layer + number_of_input_channels_from_cross_connection,
#                              number_of_output_channels_for_each_conv_block=number_of_output_channels,
#                              number_of_conv_blocks=number_of_layers,
#                              kernel_size=kernel_sizes, stride=1, bias=True, padding_type='reflect', normalization_function=None, activation_function='leakyrelu', mode='CNA',
#                              final_residual_branch_scale_factor=1/number_of_layers)
#
#
#         ### Final Projection Block: ###
#         if flag_use_final_projection_block:
#             self.projection_block = Color_Space_Conversion_Layer(number_of_input_channels=self.conv1.final_number_of_channels,number_of_output_channels=number_of_output_channels_after_projection_block)
#             self.conv1 = nn.Sequential(self.conv1, self.projection_block)
#             self.final_number_of_channels = number_of_output_channels_after_projection_block
#         else:
#             self.final_number_of_channels = self.conv1.final_number_of_channels
#
#         # if flag_sequential_concat:
#         #     self.final_number_of_channels += number_of_input_channels_from_cross_connection + number_of_input_channels_from_upper_layer
#
#         #DownSample Strategy:
#         if flag_downsample_strategy == 'maxpool':
#             self.downsample = nn.MaxPool2d(kernel_size=2)
#         elif flag_downsample_strategy == 'unshuffle':
#             self.downsample = UnshufflePixels(2); #less spatial extent more channels!!!!
#         elif flag_downsample_strategy == 'simple_downsample':
#             self.downsample = DownSample_Simple(2); #TODO: implement
#         elif flag_downsample_strategy == 'DPP_learned':
#             self.downsample = DPP_learned();
#         elif flag_downsample_strategy == 'DPP':
#             self.downsample = DPP();
#         elif flag_downsample_strategy == 'avgpool':
#             self.downsample = nn.AvgPool2d(kernel_size=2);
#
#
#     def forward(self, x, outside_connection_input=None, y_cell_conditional=None, y_deformable_conditional=None):
#         ### if there is a cross connection (or outside input) -> concat it to input: ###
#         if self.flag_use_cross_connection and outside_connection_input is not None:
#             output_before_maxpool = torch.cat([outside_connection_input, x], dim=1)
#         else:
#             output_before_maxpool = x;
#
#         ### Pass Forward through the layers: ###
#         # print('output_before_maxpool: ' + str(output_before_maxpool.shape))
#         output_before_maxpool = self.conv1(output_before_maxpool, y_cell_conditional, y_deformable_conditional)
#
#         ### Prepare output to lower layer: ###
#         output_after_maxpool = self.downsample(output_before_maxpool)
#         if self.flag_add_unshuffled_input_to_lower_level:
#             unshuffled_input = UnshufflePixels(x,2)
#             output_after_maxpool = torch.cat([output_after_maxpool, unshuffled_input], dim=1)
#
#         return output_before_maxpool, output_after_maxpool










######################################################################################################################################################################################################################################################################################################################
class UNET_V3(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self):
        super(UNET_V3, self).__init__()


        ### Things To Check: ###
        '''
        (1). reduce skip connectionss
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = None

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                  number_of_output_channels=number_of_channels_Encoder1,
                                  kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                  number_of_output_channels=hidden_sizes_Encoder2,
                                  kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,32] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3,
                                  number_of_output_channels=hidden_sizes_Encoder3,
                                  kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [32,32,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = ConvGRU2D(input_size=input_from_upper_level_number_of_channels_Center,
                                  hidden_sizes=hidden_sizes_Center,
                                  n_layers=len(hidden_sizes_Center),
                                  kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flags_deformable_convolution=True)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                     number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                     number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                     number_of_output_channels=hidden_sizes_Decoder3,
                                     flag_use_cross_connection = True,
                                     kernel_sizes=[5,5], strides=1, dilations=[1,1], groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                     number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                     number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                     number_of_output_channels=hidden_sizes_Decoder2,
                                     flag_use_cross_connection=True,
                                     kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear')
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                     number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                     number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                     number_of_output_channels=output_number_of_channels_Decoder1,
                                     flag_use_cross_connection=True,
                                     kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,activation_function=final_activation_function,flag_upsample_method='bilinear')
        ###########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);


    def forward(self, x, reset_flags_list):
        x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)
        encoder3, maxpool3 = self.down3(maxpool2)

        center = self.center(maxpool3, reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center)
        decoder2 = self.up_concat2(encoder2, decoder3)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3

######################################################################################################################################################################################################################################################################################################################























######################################################################################################################################################################################################################################################################################################################
class UNET_V7(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self):
        super(UNET_V7, self).__init__()


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        self.Padding = nn.ReflectionPad2d(1)
        ###########################################################################################################################################################
        self.center = ConvGRU2D(input_size=3,
                                  hidden_sizes=[10,10,3],
                                  n_layers=3,
                                  kernel_sizes=[3,3,3], strides=1, dilations=1, groups=1, normalization_function=None, flags_deformable_convolution=False)
        self.final = nn.Conv2d(3,3,3);
        ############################################################################################################################################################



    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);


    def forward(self, x):
        center = self.center(x)
        # return self.final(self.Padding(center))
        # return self.final(self.Padding(center + x))
        # return (x+center)/2;
        return center

######################################################################################################################################################################################################################################################################################################################























######################################################################################################################################################################################################################################################################################################################
class UNET_V10(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V10, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer at both encoder and decoder. NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_hidden_states_channels=number_of_channels_Encoder1,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_hidden_states_channels=hidden_sizes_Encoder2,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,32] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3,
                                         number_of_hidden_states_channels=hidden_sizes_Encoder3,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                            number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                            number_of_hidden_states_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear', activation_function=activation_function)
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                         number_of_hidden_states_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear', activation_function=activation_function)
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                            number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                            number_of_hidden_states_channels=output_number_of_channels_Decoder1,
                                            flag_use_cross_connection=True,
                                            kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,activation_function=final_activation_function,flag_upsample_method='bilinear')
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x,reset_flags_list)
        encoder2, maxpool2 = self.down2(maxpool1,reset_flags_list)
        encoder3, maxpool3 = self.down3(maxpool2,reset_flags_list)

        center = self.center(maxpool3,reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center,reset_flags_list)
        decoder2 = self.up_concat2(encoder2, decoder3,reset_flags_list)
        decoder1 = self.up_concat1(encoder1, decoder2,reset_flags_list)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



















######################################################################################################################################################################################################################################################################################################################
class UNET_V11(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_hidden_states_channels=number_of_channels_Encoder1,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_hidden_states_channels=hidden_sizes_Encoder2,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,32] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3,
                                         number_of_hidden_states_channels=hidden_sizes_Encoder3,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                            number_of_output_channels=output_number_of_channels_Decoder1,
                                            flag_use_cross_connection=True,
                                            kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,
                                            activation_function=final_activation_function,flag_upsample_method='bilinear', initialization_method='dirac')
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x,reset_flags_list)
        encoder2, maxpool2 = self.down2(maxpool1,reset_flags_list)
        encoder3, maxpool3 = self.down3(maxpool2,reset_flags_list)

        center = self.center(maxpool3,reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center)
        decoder2 = self.up_concat2(encoder2, decoder3)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



















######################################################################################################################################################################################################################################################################################################################
class UNET_V11_2(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_2, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1, number_of_output_channels=number_of_channels_Encoder1,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2, number_of_output_channels=hidden_sizes_Encoder2,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,32] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3, number_of_output_channels=hidden_sizes_Encoder3,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [32,32]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                            number_of_output_channels=output_number_of_channels_Decoder1,
                                            flag_use_cross_connection=True,
                                            kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,
                                            activation_function=final_activation_function,flag_upsample_method='bilinear', initialization_method='dirac')
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)
        encoder3, maxpool3 = self.down3(maxpool2)

        center = self.center(maxpool3,reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center)
        decoder2 = self.up_concat2(encoder2, decoder3)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

















######################################################################################################################################################################################################################################################################################################################
class UNET_V11_3(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_3, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,64] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder3,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                            number_of_output_channels=output_number_of_channels_Decoder1,
                                            flag_use_cross_connection=True,
                                            flag_Sequential_or_RDB='sequential',
                                            flag_sequential_resnet=True,
                                            flag_sequential_concat=False,
                                            kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,
                                            activation_function=final_activation_function,flag_upsample_method='bilinear', initialization_method='dirac')
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)
        encoder3, maxpool3 = self.down3(maxpool2)

        center = self.center(maxpool3,reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center)
        decoder2 = self.up_concat2(encoder2, decoder3)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################












######################################################################################################################################################################################################################################################################################################################
class UNET_V11_4(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_4, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_3: 2 downscales instead of 3


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################










######################################################################################################################################################################################################################################################################################################################
class UNET_V11_5(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_5, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################














######################################################################################################################################################################################################################################################################################################################
class UNET_V11_6(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_6, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [3*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='simple_shuffle', #NOTE: upsample method for LOWER LEVEL....
                                            activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=False,   #NOTE: as we can see - shuffle without cross connection
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='simple_shuffle', #NOTE: upsample method for LOWER LEVEL....
                                         activation_function=final_activation_function, initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


















######################################################################################################################################################################################################################################################################################################################
class UNET_V11_7(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_7, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [3*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,  #NOTE: as we can see- bilinear with cross connection
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=final_activation_function, initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################













######################################################################################################################################################################################################################################################################################################################
class UNET_V11_8(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_8, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool', #TODO: add the possibility of simple strided convolution!!@
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [3*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='simple_shuffle', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,  #NOTE: as we can see - shuffle and cross connection
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='simple_shuffle', activation_function=final_activation_function, initialization_method='dirac')
        self.final_shuffle = ShufflePixels(2)
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return self.final_shuffle(decoder1)
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
















######################################################################################################################################################################################################################################################################################################################
class UNET_V11_9(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_9, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool', #TODO: add the possibility of simple strided convolution!!@
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [3*4*4,3*4*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4*4,3*4*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='simple_shuffle', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='simple_shuffle', activation_function=final_activation_function, initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


















######################################################################################################################################################################################################################################################################################################################
class UNET_V11_10(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V11_10, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_Sequential_or_RDB='rdb',
                                         flag_sequential_resnet=False,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [3*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_Sequential_or_RDB='rdb',
                                         flag_sequential_resnet=False,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function,
                                         flag_downsample_strategy='maxpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'rdb',
                                            flag_sequential_resnet = False,
                                            flag_sequential_concat = False,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                            flag_upsample_method='bilinear', activation_function=activation_function, initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'rdb',
                                         flag_sequential_resnet = False,
                                         flag_sequential_concat = False,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function,
                                         flag_upsample_method='bilinear', activation_function=final_activation_function, initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
























######################################################################################################################################################################################################################################################################################################################
class UNET_V12(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V12, self).__init__()


        ### UNIT_V10: No total size change Feed-Forward RDB based Net with recurrent unit at the end of each step at the encoder.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'
        # residual_branch_scale_factor = 0
        residual_branch_scale_factor = 0.2

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->3*4
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1_RDB = RDB(number_of_input_channels=3,
                             kernel_size=5,
                             number_of_output_channels_for_each_conv_block=10,
                             number_of_conv_blocks=2,
                             stride=1,dilations=1,groups=1,bias=1,
                             padding_type='reflect',
                             normalization_function='none',
                             activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.down1_unshuffle = UnshufflePixels(2);
        self.down1 = nn.Sequential(self.down1_RDB,self.down1_unshuffle)


        #(2). 3*4 -> 3*4*4
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2_RDB = RDB(number_of_input_channels = 3*4,
                             kernel_size = 5,
                             number_of_output_channels_for_each_conv_block = 10,
                             number_of_conv_blocks = 2,
                             stride=1,dilations=1,groups=1,bias=1,
                             padding_type='reflect',
                             normalization_function='none',
                             activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.down2_unshuffle = UnshufflePixels(2);
        self.down2 = nn.Sequential(self.down2_RDB,self.down2_unshuffle)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  3*4*4->3*4*4
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*4*4,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 3*4*4 -> 3*4
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2_RDB = RDB(number_of_input_channels = 3 * 4  * 4,
                                  kernel_size = 5,
                                  number_of_output_channels_for_each_conv_block=10,
                                  number_of_conv_blocks=2,
                                  stride=1, dilations=1, groups=1, bias=1,
                                  padding_type='reflect',
                                  normalization_function='none',
                                  activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.up_concat2_shuffle = ShufflePixels(2)
        self.up_concat2 = nn.Sequential(self.up_concat2_RDB,self.up_concat2_shuffle)

        #(2). 3*4 -> 3
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1_RDB = RDB(number_of_input_channels = 3 * 4,
                                  kernel_size = 5,
                                  number_of_output_channels_for_each_conv_block = 10,
                                  number_of_conv_blocks = 2,
                                  stride=1, dilations=1, groups=1, bias=1,
                                  padding_type='reflect',
                                  normalization_function='none',
                                  activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.up_concat1_shuffle = ShufflePixels(2)
        self.up_concat1 = nn.Sequential(self.up_concat1_RDB, self.up_concat1_shuffle)


        #(3). 32->3:
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1 = self.down1(x)
        encoder2 = self.down2(encoder1)

        center = self.center(encoder2,reset_flags_list)

        decoder2 = self.up_concat2(center)
        decoder1 = self.up_concat1(decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################









######################################################################################################################################################################################################################################################################################################################
class UNET_V12_3(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V12_3, self).__init__()


        ### Difference from V12 base - only one layer in the center


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'
        # residual_branch_scale_factor = 0
        residual_branch_scale_factor = 0.2

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->3*4
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1_RDB = RDB(number_of_input_channels=3,
                             kernel_size=5,
                             number_of_output_channels_for_each_conv_block=10,
                             number_of_conv_blocks=2,
                             stride=1,dilations=1,groups=1,bias=1,
                             padding_type='reflect',
                             normalization_function='none',
                             activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.down1_unshuffle = UnshufflePixels(2);
        self.down1 = nn.Sequential(self.down1_RDB,self.down1_unshuffle)


        #(2). 3*4 -> 3*4*4
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2_RDB = RDB(number_of_input_channels = 3*4,
                             kernel_size = 5,
                             number_of_output_channels_for_each_conv_block = 10,
                             number_of_conv_blocks = 2,
                             stride=1,dilations=1,groups=1,bias=1,
                             padding_type='reflect',
                             normalization_function='none',
                             activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.down2_unshuffle = UnshufflePixels(2);
        self.down2 = nn.Sequential(self.down2_RDB,self.down2_unshuffle)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  3*4*4->3*4*4
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*4*4,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 3*4*4 -> 3*4
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2_RDB = RDB(number_of_input_channels = 3 * 4  * 4,
                                  kernel_size = 5,
                                  number_of_output_channels_for_each_conv_block=10,
                                  number_of_conv_blocks=2,
                                  stride=1, dilations=1, groups=1, bias=1,
                                  padding_type='reflect',
                                  normalization_function='none',
                                  activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.up_concat2_shuffle = ShufflePixels(2)
        self.up_concat2 = nn.Sequential(self.up_concat2_RDB,self.up_concat2_shuffle)

        #(2). 3*4 -> 3
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1_RDB = RDB(number_of_input_channels = 3 * 4,
                                  kernel_size = 5,
                                  number_of_output_channels_for_each_conv_block = 10,
                                  number_of_conv_blocks = 2,
                                  stride=1, dilations=1, groups=1, bias=1,
                                  padding_type='reflect',
                                  normalization_function='none',
                                  activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=residual_branch_scale_factor,initialization_method='xavier');
        self.up_concat1_shuffle = ShufflePixels(2)
        self.up_concat1 = nn.Sequential(self.up_concat1_RDB, self.up_concat1_shuffle)


        #(3). 32->3:
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1 = self.down1(x)
        encoder2 = self.down2(encoder1)

        center = self.center(encoder2,reset_flags_list)

        decoder2 = self.up_concat2(center)
        decoder1 = self.up_concat1(decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################











######################################################################################################################################################################################################################################################################################################################
class UNET_V12_2(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V12_2, self).__init__()


        ### UNIT_V10: No total size change Feed-Forward RDB based Net with recurrent unit at the end of each step at the encoder.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->3*4
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1_RDB = RDB(3,5,10,2,1,1,1,1,'reflect',normalization_function='none', activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.down1_unshuffle = UnshufflePixels(2);
        self.down1 = nn.Sequential(self.down1_RDB,self.down1_unshuffle)


        #(2). 3*4 -> 3*4*4
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2_RDB = RDB(3*4, 5,10,2,1,1,1,1,'reflect',normalization_function='none', activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.down2_unshuffle = UnshufflePixels(2);
        self.down2 = nn.Sequential(self.down2_RDB,self.down2_unshuffle)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  3*4*4->3*4*4
        number_of_layers_Center = 3;
        # hidden_sizes_Center = [3*4*4,3*4*4]
        hidden_sizes_Center = [3*4*4*10,3*4*4]  #The change between this and UNET_V12 is the extra number of hidden states to supposedly allow learning
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*4*4,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 3*4*4 -> 3*4
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2_RDB = RDB(3 * 4  * 4, 5, 10, 2, 1, 1, 1, 1, 'reflect', normalization_function='none', activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.up_concat2_shuffle = ShufflePixels(2)
        self.up_concat2 = nn.Sequential(self.up_concat2_RDB,self.up_concat2_shuffle)

        #(2). 3*4 -> 3
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1_RDB = RDB(3 * 4, 5, 10, 2, 1, 1, 1, 1, 'reflect', normalization_function='none', activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.up_concat1_shuffle = ShufflePixels(2)
        self.up_concat1 = nn.Sequential(self.up_concat1_RDB, self.up_concat1_shuffle)


        #(3). 32->3:
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1 = self.down1(x)
        encoder2 = self.down2(encoder1)

        center = self.center(encoder2,reset_flags_list)

        decoder2 = self.up_concat2(center)
        decoder1 = self.up_concat1(decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
















######################################################################################################################################################################################################################################################################################################################
class UNET_V13(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V13, self).__init__()


        ### UNIT_V10: No real spatial learning - unshuffling -> center with memory -> unshuffle.... should work pretty much like the stacked version of the memory unit
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->3*4
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1_RDB = RDB(3,5,10,2,1,1,1,1,'reflect',normalization_function='none', activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.down1_unshuffle = UnshufflePixels(2);
        self.down1 = nn.Sequential(self.down1_unshuffle)


        #(2). 3*4 -> 3*4*4
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2_RDB = RDB(3*4, 5,10,2,1,1,1,1,'reflect',normalization_function='none', activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.down2_unshuffle = UnshufflePixels(2);
        self.down2 = nn.Sequential(self.down2_unshuffle)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  3*4*4->3*4*4
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*4*4,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 3*4*4 -> 3*4
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2_RDB = RDB(3 * 4  * 4, 5, 10, 2, 1, 1, 1, 1, 'reflect', normalization_function='none', activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.up_concat2_shuffle = ShufflePixels(2)
        self.up_concat2 = nn.Sequential(self.up_concat2_shuffle)

        #(2). 3*4 -> 3
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1_RDB = RDB(3 * 4, 5, 10, 2, 1, 1, 1, 1, 'reflect', normalization_function='none', activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.up_concat1_shuffle = ShufflePixels(2)
        self.up_concat1 = nn.Sequential(self.up_concat1_shuffle)


        #(3). 32->3:
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1 = self.down1(x)
        encoder2 = self.down2(encoder1)

        center = self.center(encoder2,reset_flags_list)

        decoder2 = self.up_concat2(center)
        decoder1 = self.up_concat1(decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################





















######################################################################################################################################################################################################################################################################################################################
class UNET_V13_2(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V13_2, self).__init__()


        ### UNIT_V10: No real spatial learning - unshuffling -> center with memory -> unshuffle.... should work pretty much like the stacked version of the memory unit
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->3*4
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1_RDB = RDB(3,5,10,2,1,1,1,1,'reflect',normalization_function='none', activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.down1_unshuffle = UnshufflePixels(2);
        self.down1 = nn.Sequential(self.down1_unshuffle)


        #(2). 3*4 -> 3*4*4
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2_RDB = RDB(3*4, 5,10,2,1,1,1,1,'reflect',normalization_function='none', activation_function='leakyrelu', mode='CNA',
                             final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.down2_unshuffle = UnshufflePixels(2);
        self.down2 = nn.Sequential(self.down2_unshuffle)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  3*4*4->3*4*4
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*4*4,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        self.center = nn.Conv2d(3*4*4,3*4*4,1)
        nn.init.dirac_(self.center.weight)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 3*4*4 -> 3*4
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2_RDB = RDB(3 * 4  * 4, 5, 10, 2, 1, 1, 1, 1, 'reflect', normalization_function='none', activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.up_concat2_shuffle = ShufflePixels(2)
        self.up_concat2 = nn.Sequential(self.up_concat2_shuffle)

        #(2). 3*4 -> 3
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1_RDB = RDB(3 * 4, 5, 10, 2, 1, 1, 1, 1, 'reflect', normalization_function='none', activation_function='leakyrelu', mode='CNA',
                                  final_residual_branch_scale_factor=0.2,initialization_method='xavier');
        self.up_concat1_shuffle = ShufflePixels(2)
        self.up_concat1 = nn.Sequential(self.up_concat1_shuffle)


        #(3). 32->3:
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1 = self.down1(x)
        encoder2 = self.down2(encoder1)
        center = self.center(encoder2)
        decoder2 = self.up_concat2(center)
        decoder1 = self.up_concat1(decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################





















######################################################################################################################################################################################################################################################################################################################
class UNET_V14(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14, self).__init__()


        ### UNIT_V14: 2 layer stacked Conv_Memory only at the Decoder & Center.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,32] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3,
                                         number_of_output_channels=hidden_sizes_Encoder3,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                            number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                            number_of_hidden_states_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear', activation_function=activation_function)
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                         number_of_hidden_states_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear', activation_function=activation_function)
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                            number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                            number_of_hidden_states_channels=output_number_of_channels_Decoder1,
                                            flag_use_cross_connection=True,
                                            kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,activation_function=final_activation_function,flag_upsample_method='bilinear')
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x,reset_flags_list)
        encoder2, maxpool2 = self.down2(maxpool1,reset_flags_list)
        encoder3, maxpool3 = self.down3(maxpool2,reset_flags_list)

        center = self.center(maxpool3,reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center,reset_flags_list)
        decoder2 = self.up_concat2(encoder2, decoder3,reset_flags_list)
        decoder1 = self.up_concat1(encoder1, decoder2,reset_flags_list)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################














######################################################################################################################################################################################################################################################################################################################
class UNET_V15(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V15, self).__init__()


        ### UNIT_V14: 2 layer stacked Conv_Memory only at the Decoder.
        # NO color space conversion layer


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'double_relu'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [8,8]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         kernel_sizes=[7,7], strides=[1,1], dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [16,16] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        #(3). 64->128
        input_from_upper_level_number_of_channels_Encoder3 = output_number_of_channels_Encoder2
        hidden_sizes_Encoder3 = [32,32] #final element = output number of channels from layer
        output_number_of_chanels_Encoder3 = hidden_sizes_Encoder3[-1]
        self.down3 = UNET_Down_V1(memory_unit_name = memory_unit_name,
                                         number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder3,
                                         number_of_output_channels=hidden_sizes_Encoder3,
                                         kernel_sizes=[5, 5], strides=[1, 1], dilations=[1, 1], groups=[1, 1], normalization_function=normalization_function, activation_function=activation_function)
        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_chanels_Encoder3
        number_of_layers_Center = 3;
        hidden_sizes_Center = [64,64,64]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Sequential_Conv_Block(number_of_input_channels=input_from_upper_level_number_of_channels_Center,
                              number_of_output_channels=input_from_upper_level_number_of_channels_Center,
                              kernel_sizes=[3,3], strides=1,dilations=1,groups=1,padding_type='reflect',
                              normalization_function='none',
                              activation_function='leakyrelu',
                              mode='CNA',
                              initialization_method='dirac',
                              flag_resnet=False,
                              flag_concat=False)  #TODO: Notice i can maybe use the resnet o concat options!!!!
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [64,32]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                            number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_chanels_Encoder3,
                                            number_of_hidden_states_channels=hidden_sizes_Decoder3,
                                            flag_use_cross_connection = True,
                                            kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear', activation_function=activation_function)
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [32,16]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                         number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                         number_of_hidden_states_channels=hidden_sizes_Decoder2,
                                         flag_use_cross_connection=True,
                                         kernel_sizes=[5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, flag_upsample_method='bilinear', activation_function=activation_function)
        #(3). 32->3:
        input_from_lower_level_number_of_channels_Decoder1 = output_number_of_channels_Decoder2
        input_from_lower_level_number_of_channels_after_updample_Decoder1 = 16;
        output_number_of_channels_Decoder1 = 3
        self.up_concat1 = UNET_Up_Memory_V1(memory_unit_name = memory_unit_name,
                                            number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder1,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                            number_of_hidden_states_channels=output_number_of_channels_Decoder1,
                                            flag_use_cross_connection=True,
                                            kernel_sizes=5,strides=1,dilations=1,groups=1,normalization_function=normalization_function,activation_function=final_activation_function,flag_upsample_method='bilinear')
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x,reset_flags_list)
        encoder2, maxpool2 = self.down2(maxpool1,reset_flags_list)
        encoder3, maxpool3 = self.down3(maxpool2,reset_flags_list)

        center = self.center(maxpool3,reset_flags_list)

        decoder3 = self.up_concat3(encoder3, center,reset_flags_list)
        decoder2 = self.up_concat2(encoder2, decoder3,reset_flags_list)
        decoder1 = self.up_concat1(encoder1, decoder2,reset_flags_list)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



























######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V1(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V1, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=True,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=True,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[5,5,5],[5,5,5]], strides=[1,1], dilations=[[1,2,4],[1,2,4]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=True,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=True,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[5,5,5],[5,5,5]], strides=[1, 1], dilations=[[1,2,4],[1,2,4]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat3 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=output_number_of_channels_Encoder2,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=True,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=True,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[5,5,5],[5,5,5]], strides=1, dilations=[[1,2,4],[1,2,4]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Decoder1,
                                         number_of_lower_level_channels_after_upsample=output_number_of_channels_Decoder1,
                                         number_of_cross_connection_channels=output_number_of_channels_Encoder1,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = hidden_sizes_Decoder2[-1],
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=True,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[5,5,5],[5,5,5]], strides=1, dilations=[[1,2,4],[1,2,4]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat3(encoder2, center)
        decoder1 = self.up_concat2(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



























######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V2(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V2, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=True,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[5,5,5],[5,5,5]], strides=[1,1], dilations=[[1,2,4],[1,2,4]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4*4,3*4*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=True,
                                         kernel_sizes=[[5,5,5],[5,5,5]], strides=[1, 1], dilations=[[1,2,4],[1,2,4]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4*4,3*4*4*4,3*4*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4*4,3*4*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = True,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[5,5,5],[5,5,5]], strides=1, dilations=[[1,2,4],[1,2,4]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = True,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[5,5,5],[5,5,5],[3]], strides=1, dilations=[[1,2,4],[1,2,4],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################











######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V2(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V2, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='avgpool',    #DPP_learned
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='avgpool',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################








######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V3(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V3, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################













######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V4(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V4, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################









######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V5(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V5, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method='dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################














######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V52(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V52, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method='dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method='dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################















######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V6(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V6, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function='batch_normalization',
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function='batch_normalization',
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function='batch_normalization',
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function='batch_normalization',
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################










######################################################################################################################################################################################################################################################################################################################
class UNET_V14_V7(nn.Module):
    #Shtik: a very simple concept - the first level of encoder and decoder are simply double Convs, and the other layers are simple ConvGRU2D.
    #       the downsampling is simple MaxPool and upsampling are simple UpsampleBillinear
    #       the Cross-Connections are simple additions without convolutions
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V14_V7, self).__init__()


        ### UNIT_V10: 2 layer stacked Conv_Memory at each and every layer only at encoder.
        # NO color space conversion layer

        ### Difference from V11_4: no cross connection to force the network to learn temporal connections instead of spatial


        ### Things To Check: ###
        '''
        (1). reduce skip connections
        (2). try FIRE blocks instead of regular convolutional blocks
        (3). smart downscale/upscale strategies
        (4). better memory layers: lstm, better gates (smarter/better learning activations?), new ideas, new/smarter information fusion between HS and input, attention
        (5). explicit optical flow 
        (6). deformable convolution
        (7). dilated convolutions
        (8). longer explicit hidden state horizon: h(t), h(t-1), ...., h(t-n)   instead of h(t) only
        (9). add convolutions to skip connections
        (10). reduce spatial extent of skip connection - downscale skip connection then upscale it 
        (11). replace max-pooling at downscale layers with strides / something else
        '''


        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4*4,3*4*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function='none',
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function='none',
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function='layer_normalization', activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function='none',
                                            flag_upsample_method='my_smart_shuffle_stacked',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4*4,3*4*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function='none',
                                         flag_upsample_method='my_smart_shuffle_stacked',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################






# x = torch.Tensor(randn(1,3,100,100))
# G_network = UNET_V14_V2();
# output_tensor = G_network(x,[1])





















######################################################################################################################################################################################################################################################################################################################
class UNET_V16_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V16_V1, self).__init__()

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3],[3]], strides=[1,1], dilations=[[1],[1]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',    #DPP_learned
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3],[3]], strides=[1, 1], dilations=[[1],[1]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3],[3]], strides=1, dilations=[[1],[1]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3],[3],[3]], strides=1, dilations=[[1],[1],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        center = self.center(maxpool2,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



























######################################################################################################################################################################################################################################################################################################################
class UNET_V17_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V17_V1, self).__init__()

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3],[3]], strides=[1,1], dilations=[[1],[1]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',    #DPP_learned
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3],[3]], strides=[1, 1], dilations=[[1],[1]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center + 3*(4**2),  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3],[3]], strides=1, dilations=[[1],[1]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3],[3],[3]], strides=1, dilations=[[1],[1],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        shuffled_input = self.unshuffle1(x)
        shuffled_input = self.unshuffle2(shuffled_input)
        center_input = torch.cat([shuffled_input, maxpool2],dim=1)
        center = self.center(center_input,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


















######################################################################################################################################################################################################################################################################################################################
class UNET_V18_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V18_V1, self).__init__()

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3],[3]], strides=[1,1], dilations=[[1],[1]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',    #DPP_learned
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3],[3]], strides=[1, 1], dilations=[[1],[1]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center + 3*(4**2),  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3],[3]], strides=1, dilations=[[1],[1]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='my_smart_shuffle',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method='dirac')
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = True,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3],[3],[3]], strides=1, dilations=[[1],[1],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='my_smart_shuffle',
                                         activation_function=final_activation_function,
                                         initialization_method='dirac')

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        shuffled_input = self.unshuffle1(x)
        shuffled_input = self.unshuffle2(shuffled_input)
        center_input = torch.cat([shuffled_input, maxpool2],dim=1)
        center = self.center(center_input,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
















######################################################################################################################################################################################################################################################################################################################
class UNET_V19_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V19_V1, self).__init__()

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3],[3]], strides=[1,1], dilations=[[1],[1]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',    #DPP_learned
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3],[3]], strides=[1, 1], dilations=[[1],[1]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*(4**2),  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = False,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3],[3]], strides=1, dilations=[[1],[1]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='simple_shuffle',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['dirac','dirac'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = False,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3],[3],[3]], strides=1, dilations=[[1],[1],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='simple_shuffle',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        shuffled_input = self.unshuffle1(x)
        shuffled_input = self.unshuffle2(shuffled_input)
        # center_input = torch.cat([shuffled_input, maxpool2*0],dim=1)
        center = self.center(shuffled_input,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################























######################################################################################################################################################################################################################################################################################################################
class UNET_V19_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V19_V1, self).__init__()

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3],[3]], strides=[1,1], dilations=[[1],[1]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',    #DPP_learned
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         number_of_input_channels_from_cross_connection=0,
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3],[3]], strides=[1, 1], dilations=[[1],[1]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP',
                                         initialization_method = 'dirac',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=3*(4**2),  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels,
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense=False,
                                            flag_sequential_resnet = False,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3],[3]], strides=1, dilations=[[1],[1]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='simple_shuffle',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['dirac','dirac'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = False,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3],[3],[3]], strides=1, dilations=[[1],[1],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='simple_shuffle',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);

        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        encoder1, maxpool1 = self.down1(x)
        encoder2, maxpool2 = self.down2(maxpool1)

        shuffled_input = self.unshuffle1(x)
        shuffled_input = self.unshuffle2(shuffled_input)
        # center_input = torch.cat([shuffled_input, maxpool2*0],dim=1)
        center = self.center(shuffled_input,reset_flags_list)

        decoder2 = self.up_concat2(encoder2, center)
        decoder1 = self.up_concat1(encoder1, decoder2)
        return decoder1
        # return conv3
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################















######################################################################################################################################################################################################################################################################################################################
class UNET_V20_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V20_V1, self).__init__()


        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d_running_average',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        #TODO: i can decide to insert to the encoder layers not only the running_average output but the concatenated running_average + regular RNN output!....maybe later
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + int(running_average_input/(4**1)),  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + int(running_average_input/(4**2)),
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);


        ### UNET: ###
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)
        #(2). Center:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        #(3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


















######################################################################################################################################################################################################################################################################################################################
class UNET_V20_V2(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V20_V2, self).__init__()


        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d_running_average',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        #TODO: i can decide to insert to the encoder layers not only the running_average output but the concatenated running_average + regular RNN output!....maybe later
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + int(running_average_input/(4**1)) + output_number_of_channels_Center,  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = False,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + int(running_average_input/(4**2)) + output_number_of_channels_Center,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);


        ### UNET: ###
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)
        #(2). Center:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(center_regular_output)
        #(3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_upsampled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

























######################################################################################################################################################################################################################################################################################################################
class UNET_V21_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V21_V1, self).__init__()


        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d_running_average',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        #TODO: i can decide to insert to the encoder layers not only the running_average output but the concatenated running_average + regular RNN output!....maybe later
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + int(running_average_input/(4**1)),  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + int(running_average_input/(4**2)),
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);


        ### UNET: ###
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)
        #(2). Center:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        #(3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################





















######################################################################################################################################################################################################################################################################################################################
class UNET_V21_V2(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V21_V2, self).__init__()


        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        #TODO: i can decide to insert to the encoder layers not only the running_average output but the concatenated running_average + regular RNN output!....maybe later
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + int(running_average_input/(4**1)),  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + int(running_average_input/(4**2)),
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);


        ### UNET: ###
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)
        #(2). Center:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        #(3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################





















######################################################################################################################################################################################################################################################################################################################
class UNET_V22_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V22_V1, self).__init__()
        ### (*). Difference from UNET_V21_V1 - i concatenate both running_average and lstm to decoder

        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d_running_average',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + int(running_average_input/(4**1)) + output_number_of_channels_Center,  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + int(running_average_input/(4**2)) + output_number_of_channels_Center,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);


        ### UNET: ###
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)
        #(2). Center:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(center_regular_output)
        #(3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_upsampled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################





















######################################################################################################################################################################################################################################################################################################################
class UNET_V22_V2(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V22_V2, self).__init__()
        ### (*). Difference from UNET_V21_V1 - i concatenate both running_average and lstm to decoder

        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + int(running_average_input/(4**1)) + output_number_of_channels_Center,  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + int(running_average_input/(4**2)) + output_number_of_channels_Center,
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);


        ### UNET: ###
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)
        #(2). Center:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        # print(x.shape)
        # print(maxpool1.shape)
        # print(maxpool2.shape)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(center_regular_output)
        #(3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_upsampled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################























































######################################################################################################################################################################################################################################################################################################################
class UNET_V23_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V23_V1, self).__init__()


        #TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        #Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        #Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3,number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        #Encoder Levels:
        #(1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3*4,3*4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                         number_of_input_channels_from_cross_connection = 0,
                                         number_of_output_channels=number_of_channels_Encoder1,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                         flag_use_cross_connection=False,
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         stack_residual_scale=1,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1,1], dilations=[[1,2,3],[1,2,3]], groups=[1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',    #DPP_learned
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)
        #(2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3*4*4,3*4*4] #final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                         flag_use_cross_connection=True,  #I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                         number_of_input_channels_from_cross_connection=3*(4**1),
                                         number_of_output_channels=hidden_sizes_Encoder2,
                                         flag_use_final_projection_block=False,
                                         number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                         flag_Sequential_or_RDB='sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet=True,
                                         flag_sequential_concat=False,
                                         kernel_sizes=[[3,3,3],[3,3,3]], strides=[1, 1], dilations=[[1,2,3],[1,2,3]], groups=[1, 1],
                                         normalization_function=normalization_function,
                                         activation_function=activation_function,
                                         flag_downsample_strategy='DPP_learned',
                                         initialization_method = 'xavier',
                                         flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################


        ###########################################################################################################################################################
        #Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        #TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3*4*4,3*4*4,3*4*4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name = memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  #2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5,5,5], strides=1, dilations=1, groups=1, normalization_function=normalization_function, activation_function=activation_function)

        running_average_input = 3*(4**2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name = 'convmean2d_running_average',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1, normalization_function='none', activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################


        ###########################################################################################################################################################
        #Decoder Levels:
        #(1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3*4*4,3*4*4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                            number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                            number_of_cross_connection_channels=self.down2.final_number_of_channels + 2*(int(running_average_input/(4**1)) + output_number_of_channels_Center),  #Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                            number_of_output_channels=hidden_sizes_Decoder3,
                                            flag_use_final_projection_block=False,
                                            number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                            flag_use_cross_connection = True,
                                            flag_Sequential_or_RDB = 'sequential',
                                            flag_sequential_dense = False,
                                            flag_sequential_resnet = True,
                                            flag_sequential_concat = False,
                                            stack_residual_scale = 1,
                                            kernel_sizes=[[3,3,3],[3,3,3]], strides=1, dilations=[[1,2,3],[1,2,3]], groups=1,
                                            normalization_function=normalization_function,
                                            flag_upsample_method='bilinear',
                                            flag_add_unshuffled_input_to_upper_level=False,
                                            activation_function=activation_function,
                                            initialization_method=['xavier','xavier'])
        #(2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3*4,3*4,3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                         number_of_cross_connection_channels=self.down1.final_number_of_channels + 2*(int(running_average_input/(4**2)) + output_number_of_channels_Center),
                                         number_of_output_channels=hidden_sizes_Decoder2,
                                         flag_use_final_projection_block = False,
                                         number_of_output_channels_after_projection_block = 3,
                                         flag_use_cross_connection=True,
                                         flag_Sequential_or_RDB = 'sequential',
                                         flag_sequential_dense=False,
                                         flag_sequential_resnet = True,
                                         flag_sequential_concat = False,
                                         stack_residual_scale = 1,
                                         kernel_sizes=[[3,3,3],[3,3,3],[3]], strides=1, dilations=[[1,2,3],[1,2,3],[1]], groups=1,
                                         normalization_function=normalization_function,
                                         flag_upsample_method='bilinear',
                                         activation_function=final_activation_function,
                                         initialization_method=['dirac','dirac','dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)
        ##########################################################################################################################################################


    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self,device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);

        #############
        ### UNET: ###
        #############
        #(1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)

        #(2). Center:
        #   (2.1). Previous RNN Outputs:
        center_running_average_output_previous = self.center_running_average.hidden_states[-1]
        center_regular_output_previous = self.center.hidden_H[-1]
        #   (2.2). Current RNN Outputs:
        center_running_average_output = self.center_running_average(unshuffled_input2,reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        #   (2.3). Upsample RNN Outputs:
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(center_regular_output)
        #(3). Decoder:
        #   (3.1). Preprocess RNN outputs for Decoder Layers:
        if center_running_average_output_previous[0] is None:
            center_running_average_output_previous[0] = torch.zeros_like(center_running_average_output)
        if center_regular_output_previous[0] is None:
            center_regular_output_previous = torch.zeros_like(center_regular_output_previous)
        center_running_average_output_previous_shuffled1 = self.shuffle1(center_running_average_output_previous)
        center_running_average_output_previous_shuffled2 = self.shuffle2(center_running_average_output_previous_shuffled1)
        center_regular_output_previous_upsampled1 = self.upsample1(center_regular_output_previous)
        center_regular_output_previous_upsampled2 = self.upsample2(center_regular_output_previous)
        #   (3.2). Decoder Layers:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_upsampled1, center_running_average_output_previous_shuffled1, center_regular_output_previous_upsampled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, center_running_average_output_previous_shuffled2, center_regular_output_previous_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)


        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################












######################################################################################################################################################################################################################################################################################################################
class UNET_V23_V2(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V23_V2, self).__init__()

        # TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        # Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        # Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        # Encoder Levels:
        # (1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3 * 4, 3 * 4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                                 number_of_input_channels_from_cross_connection=0,
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=False,
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP_learned',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False)
        # (2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4, 3 * 4 * 4]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                                 flag_use_cross_connection=True,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 number_of_input_channels_from_cross_connection=3 * (4 ** 1),
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP_learned',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################

        ###########################################################################################################################################################
        # Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        # TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4, 3 * 4 * 4, 3 * 4 * 4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function)

        running_average_input = 3 * (4 ** 2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1, kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################

        ###########################################################################################################################################################
        # Decoder Levels:
        # (1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4, 3 * 4 * 4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                             number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                             number_of_cross_connection_channels=self.down2.final_number_of_channels + 2*(int(running_average_input / (4 ** 1)) + output_number_of_channels_Center),  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'])
        # (2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4, 3 * 4, 3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_cross_connection_channels=self.down1.final_number_of_channels + 2*(int(running_average_input / (4 ** 2)) + output_number_of_channels_Center),
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);

        #############
        ### UNET: ###
        #############
        # (1). Encoder:
        encoder1, maxpool1 = self.down1(x=x, outside_connection_input=None)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)

        # (2). Center:
        #   (2.1). Previous RNN Outputs:
        center_running_average_output_previous = self.center_running_average.hidden_states[-1]
        center_regular_output_previous = self.center.hidden_H[-1]
        #   (2.2). Current RNN Outputs:
        center_running_average_output = self.center_running_average(unshuffled_input2, reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        #   (2.3). Upsample RNN Outputs:
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(center_regular_output)
        # (3). Decoder:
        #   (3.1). Preprocess RNN outputs for Decoder Layers:
        if center_running_average_output_previous[0] is None:
            #TODO: initialize as unshuffled input as well
            center_running_average_output_previous[0] = torch.zeros_like(center_running_average_output)
        if center_regular_output_previous[0] is None:
            center_regular_output_previous = torch.zeros_like(center_regular_output_previous)
        center_running_average_output_previous_shuffled1 = self.shuffle1(center_running_average_output_previous)
        center_running_average_output_previous_shuffled2 = self.shuffle2(center_running_average_output_previous_shuffled1)
        center_regular_output_previous_upsampled1 = self.upsample1(center_regular_output_previous)
        center_regular_output_previous_upsampled2 = self.upsample2(center_regular_output_previous)
        #   (3.2). Decoder Layers:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_upsampled1, center_running_average_output_previous_shuffled1, center_regular_output_previous_upsampled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, center_running_average_output_previous_shuffled2, center_regular_output_previous_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



















######################################################################################################################################################################################################################################################################################################################
class UNET_V23_V2(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V23_V2, self).__init__()

        # TODO: add possibility/flag for deformable convolution in the Down and Up blocks as well as the Sequential_General blocks

        # Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'

        # Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        # Encoder Levels:
        # (1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3 * 4, 3 * 4]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1,
                                                 number_of_input_channels_from_cross_connection=0,
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=False,
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP_learned',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False)
        # (2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4, 3 * 4 * 4]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2,
                                                 flag_use_cross_connection=True,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 number_of_input_channels_from_cross_connection=3 * (4 ** 1),
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP_learned',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False)

        ###########################################################################################################################################################

        ###########################################################################################################################################################
        # Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        # TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4, 3 * 4 * 4, 3 * 4 * 4]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.center = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function)

        running_average_input = 3 * (4 ** 2)
        self.center_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none')
        self.center_running_average2 = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none')

        self.center_output_size = hidden_sizes_Center[-1] + running_average_input;
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################

        ###########################################################################################################################################################
        # Decoder Levels:
        # (1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4, 3 * 4 * 4]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=output_number_of_channels_Center,
                                             number_of_lower_level_channels_after_upsample=output_number_of_channels_Center,
                                             number_of_cross_connection_channels=self.down2.final_number_of_channels + 2*(int(running_average_input / (4 ** 1)) + output_number_of_channels_Center),  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'])
        # (2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4, 3 * 4, 3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_cross_connection_channels=self.down1.final_number_of_channels + 2*(int(running_average_input / (4 ** 2)) + output_number_of_channels_Center), #TODO: understand exactly what's needed
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);

        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if center_running_average_output_previous[0] is None:
            center_running_average_output_previous = unshuffled_input2;
        if center_regular_output_previous[0] is None:
            center_regular_output_previous = torch.zeros([center_regular_output_previous.shape[0],1*center_regular_output_previous.shape[1],center_regular_output_previous.shape[2],center_regular_output_previous.shape[3]])
        ### Shuffle / Upample Previous RNN Outputs: ##
        center_running_average_output_previous_shuffled1 = self.shuffle1(center_running_average_output_previous)
        center_running_average_output_previous_shuffled2 = self.shuffle2(center_running_average_output_previous_shuffled1)
        center_regular_output_previous_upsampled1 = self.upsample1(center_regular_output_previous)
        center_regular_output_previous_upsampled2 = self.upsample2(center_regular_output_previous)


        # (1). Encoder:
        encoder1_input = torch.cat([x,center_running_average_output_previous_shuffled1,center_regular_output_previous_upsampled1], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=None)
        encoder2_input = torch.cat([maxpool1,center_running_average_output_previous_shuffled2,center_regular_output_previous_upsampled2], dim=1)
        encoder2, maxpool2 = self.down2(x=maxpool1, outside_connection_input=unshuffled_input1)

        # (2). Center:
        #   (2.1). Previous RNN Outputs:
        center_running_average_output_previous = self.center_running_average.hidden_states[-1]
        center_regular_output_previous = self.center.hidden_H[-1]
        #   (2.2). Current RNN Outputs:
        center_running_average_output = self.center_running_average(unshuffled_input2, reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_regular_output_running_average = self.center_running_average2(center_regular_output)
        combined_center_regular_output = torch.cat([center_regular_output, center_regular_output_running_average], dim=1)
        #   (2.3). Upsample Current RNN Outputs:
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(combined_center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(combined_center_regular_output)

        # (3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_upsampled1, center_running_average_output_previous_shuffled1, center_regular_output_previous_upsampled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, center_running_average_output_previous_shuffled2, center_regular_output_previous_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=center_regular_output)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

































######################################################################################################################################################################################################################################################################################################################
class UNET_V24_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V24_V1, self).__init__()
        #(*). ENTER DEFORMABLE CONVOLUTIONS! + INPUT INTENSITY EXPLICITELY!

        # Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 2;

        # Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        # Encoder Levels:
        # (1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.channels_from_center = 384; ### Currently this isn't automatic because it requires access to future knowledge of number of channels
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1 + self.channels_from_center + number_of_initial_conv_output_color_channels, #raw input + channels_from_center + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=0,
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=False,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[True, True, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False])

        # (2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2 + self.channels_from_center + number_of_initial_conv_output_color_channels*4, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=3 * (4 ** 1),
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=True,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[True, True, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False])

        ###########################################################################################################################################################




        ###########################################################################################################################################################
        # Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        # TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center

        running_average_input = 3 * (4 ** 2)
        #(1). raw unshuffled RGB convmean2d output:
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False)
        # (2). different output activations of regular LSTM-Flags output:
        self.center = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[True, True, True],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, False])
        self.center_running_average_of_lstm_output = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=hidden_sizes_Center[-1],
                                                      hidden_sizes=hidden_sizes_Center[-1],
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution = False,
                                                      flag_deformable_convolution_version = 'v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution = 'before',
                                                      flag_deformable_convolution_modulation = False)
        self.center_alpha_estimator_of_lstm_output = Get_Memory_Unit(memory_unit_name='alpha_estimator',
                                                       input_size=hidden_sizes_Center[-1],
                                                       hidden_sizes=hidden_sizes_Center[-1],
                                                       n_layers=1,
                                                       kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                       normalization_function='none',
                                                       activation_function='none',
                                                       initialization_method='xavier',
                                                       flag_deformable_convolution=False,
                                                       flag_deformable_convolution_version='v1',
                                                       flag_deformable_convolution_before_or_after_main_convolution='before',
                                                       flag_deformable_convolution_modulation=False)
        self.center_running_sum_of_lstm_output = Get_Memory_Unit(memory_unit_name='convsum2d',
                                                       input_size=hidden_sizes_Center[-1],
                                                       hidden_sizes=hidden_sizes_Center[-1],
                                                       n_layers=1,
                                                       kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                       normalization_function='none',
                                                       activation_function='none',
                                                       initialization_method='xavier',
                                                       flag_deformable_convolution=False,
                                                       flag_deformable_convolution_version='v1',
                                                       flag_deformable_convolution_before_or_after_main_convolution='before',
                                                       flag_deformable_convolution_modulation=False)


        self.center_output_size = 4*hidden_sizes_Center[-1];
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################




        ###########################################################################################################################################################
        # Decoder Levels:
        # (1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.center_output_size,
                                             number_of_lower_level_channels_after_upsample=self.center_output_size,
                                             number_of_cross_connection_channels=self.down2.final_number_of_channels + 2*int(running_average_input/4) + self.center_output_size,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[True, True, True],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, False])
        # (2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        # print('down1 = ' + str(self.down1.final_number_of_channels))
        # print('running average input * 2 = ' + str(2*(int(running_average_input/16))))
        # print('center output size * 2 = ' + str(2*self.center_output_size))
        # print('input cross connection = ' + str(self.down1.final_number_of_channels + 2*(int(running_average_input/16) + 2*self.center_output_size)))
        # print('input from lower level = ' + str(input_from_lower_level_number_of_channels_Decoder2))
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_cross_connection_channels=self.down1.final_number_of_channels + 2*(int(running_average_input/16)) + 2*self.center_output_size, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [True,True,True],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(2);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_running_average_output_previous = None
        self.center_regular_output_previous = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);

        # ### Previous RNN Outputs: ###
        # self.center_running_average_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_regular_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if self.center_running_average_output_previous is None or reset_flags_list[0]==1:
            self.center_running_average_output_previous = unshuffled_input2;
        if self.center_regular_output_previous is None or reset_flags_list[0]==1:
            self.center_regular_output_previous = torch.zeros(x.shape[0], self.hidden_sizes_Center[-1]*4, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
        if reset_flags_list[0] == 3:
            self.center_regular_output_previous = self.center_regular_output_previous.detach()
            self.center_running_average_output_previous = self.center_running_average_output_previous.detach()

        ### Shuffle / Upample Previous RNN Outputs: ##
        center_running_average_output_previous_shuffled1 = self.shuffle1(self.center_running_average_output_previous)
        center_running_average_output_previous_shuffled2 = self.shuffle2(center_running_average_output_previous_shuffled1)
        center_regular_output_previous_upsampled1 = self.upsample1(self.center_regular_output_previous)
        center_regular_output_previous_upsampled2 = self.upsample2(self.center_regular_output_previous)


        # (1). Encoder:
        # print('x = ' + str(x.shape))
        # print('center_running_average_output_previous_shuffled1 = ' + str(center_running_average_output_previous_shuffled1.shape))
        # print('center_regular_output_previous_upsampled1 = ' + str(center_regular_output_previous_upsampled1.shape))
        # print('center_running_average_output_previous_shuffled2 = ' + str(center_running_average_output_previous_shuffled2.shape))
        # print('center_regular_output_previous_upsampled2 = ' + str(center_regular_output_previous_upsampled2.shape))
        #TODO: this is a mess.... have clear rules as to what to insert as "outside_connection_input" and what to put into "main" input
        encoder1_input = torch.cat([x,center_running_average_output_previous_shuffled2,center_regular_output_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=None)  #TODO: insert center_running_average_output_previous_shuffled2 and the second one as "outside_connection_input" to be able to use flags to use it or not
        encoder2_input = torch.cat([maxpool1,center_running_average_output_previous_shuffled1,center_regular_output_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=unshuffled_input1)  #input raw RGB unshuffled as outside_connection_input


        # (2). Center:
        #   (2.2). Current RNN Outputs:
        center_running_average_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_regular_output_running_average = self.center_running_average_of_lstm_output(center_regular_output)
        center_regular_output_alpha_estimator = self.center_alpha_estimator_of_lstm_output(center_regular_output)
        center_regular_output_running_sum = self.center_running_sum_of_lstm_output(center_regular_output)
        combined_center_regular_output = torch.cat([center_regular_output, center_regular_output_running_average, center_regular_output_alpha_estimator, center_regular_output_running_sum], dim=1)
        #   (2.3). Assign previous outputs for next round:
        self.center_running_average_output_previous = center_running_average_output
        self.center_regular_output_previous = combined_center_regular_output
        #   (2.3). Upsample Current RNN Outputs:
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output_shuffled1)
        center_regular_output_upsampled1 = self.upsample1(combined_center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(combined_center_regular_output)


        # (3). Decoder:
        decoder2_cross_connection = torch.cat([center_running_average_output_shuffled1, center_regular_output_previous_upsampled1, center_running_average_output_previous_shuffled1, encoder2], dim=1)
        decoder1_cross_connection = torch.cat([center_running_average_output_shuffled2, center_regular_output_upsampled2, center_running_average_output_previous_shuffled2, center_regular_output_previous_upsampled2, encoder1], dim=1)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=combined_center_regular_output)

        # print(center_running_average_output_shuffled2.shape)
        # print(center_regular_output_upsampled2.shape)
        # print(center_running_average_output_previous_shuffled2.shape)
        # print(center_regular_output_previous_upsampled2.shape)
        # print(encoder1.shape)
        # print(decoder2.shape)
        # print(decoder1_cross_connection.shape)

        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder2)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



























######################################################################################################################################################################################################################################################################################################################
class UNET_V25_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V25_V1, self).__init__()
        #(*). ENTER DEFORMABLE CONVOLUTIONS! + INPUT INTENSITY EXPLICITELY!

        # Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 2;

        # Pre-Encoder:
        number_of_initial_conv_output_color_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, number_of_initial_conv_output_color_channels)

        ###########################################################################################################################################################
        # Encoder Levels:
        # (1). 3->32
        input_from_upper_level_number_of_channels_Encoder1 = number_of_initial_conv_output_color_channels;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder1 + number_of_initial_conv_output_color_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=self.number_of_channels_LSTM_CCL_to_encoder_next_time,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=True,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[True, True, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False])

        # (2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]

        #   (1.2). Encoder2:
        # encoder2_input = torch.cat([maxpool1, center_running_average_output_previous_shuffled1, ], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_regular_output_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        # encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2 + number_of_initial_conv_output_color_channels*4, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=number_of_initial_conv_output_color_channels*4 + self.number_of_channels_LSTM_CCL_to_encoder_next_time,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=True,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 3], [1, 2, 3]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[True, True, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False])

        ###########################################################################################################################################################




        ###########################################################################################################################################################
        # Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        # TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center

        ### Number of channels: ###
        running_average_input = 3 * (4 ** 2)

        #(1). raw unshuffled RGB convmean2d output:
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False)
        # (2). different output activations of regular LSTM-Flags output:
        #   (2.1). LSTM_flags:
        self.center = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[True, True, True],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, False])
        #   (2.2). LSTM_flags -> running_average:
        self.center_running_average_of_lstm_output = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=hidden_sizes_Center[-1],
                                                      hidden_sizes=hidden_sizes_Center[-1],
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution = False,
                                                      flag_deformable_convolution_version = 'v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution = 'before',
                                                      flag_deformable_convolution_modulation = False)
        #   (2.3). LSTM_flags -> alpha_estimator:
        self.center_alpha_estimator_of_lstm_output = Get_Memory_Unit(memory_unit_name='alpha_estimator',
                                                       input_size=hidden_sizes_Center[-1],
                                                       hidden_sizes=hidden_sizes_Center[-1],
                                                       n_layers=1,
                                                       kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                       normalization_function='none',
                                                       activation_function='none',
                                                       initialization_method='xavier',
                                                       flag_deformable_convolution=False,
                                                       flag_deformable_convolution_version='v1',
                                                       flag_deformable_convolution_before_or_after_main_convolution='before',
                                                       flag_deformable_convolution_modulation=False)
        #   (2.4). LSTM_flags -> running_sum:
        self.center_running_sum_of_lstm_output = Get_Memory_Unit(memory_unit_name='convsum2d',
                                                       input_size=hidden_sizes_Center[-1],
                                                       hidden_sizes=hidden_sizes_Center[-1],
                                                       n_layers=1,
                                                       kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                       normalization_function='none',
                                                       activation_function='none',
                                                       initialization_method='xavier',
                                                       flag_deformable_convolution=False,
                                                       flag_deformable_convolution_version='v1',
                                                       flag_deformable_convolution_before_or_after_main_convolution='before',
                                                       flag_deformable_convolution_modulation=False)

        self.number_of_heads_on_top_of_basic_LSTM = 4 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        # self.channels_from_center = 384;
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        # self.center_modules_list = nn.ModuleList([self.center_running_average, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################




        ###########################################################################################################################################################
        # Decoder Levels:
        # (1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]

        # decoder2_lstm_inputs = torch.cat([center_regular_output_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_running_average_output_shuffled1, center_running_average_output_previous_shuffled1], dim=1)
        # decoder2_direct_input = torch.cat([combined_center_regular_output_to_decoder], dim=1)
        # decoder2_cross_connection = torch.cat([decoder2_running_average_inputs, decoder2_lstm_inputs, encoder2], dim=1)
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)

        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.number_of_channels_LSTM_CCL_to_decoder,
                                             number_of_lower_level_channels_after_upsample=self.number_of_channels_LSTM_CCL_to_decoder,
                                             number_of_cross_connection_channels=self.down2.final_number_of_channels + 2*int(running_average_input/4) + self.number_of_channels_LSTM_CCL_to_decoder,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[True, True, True],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, False])
        # (2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        # print('down1 = ' + str(self.down1.final_number_of_channels))
        # print('running average input * 2 = ' + str(2*(int(running_average_input/16))))
        # print('center output size * 2 = ' + str(2*self.center_output_size))
        # print('input cross connection = ' + str(self.down1.final_number_of_channels + 2*(int(running_average_input/16) + 2*self.center_output_size)))
        # print('input from lower level = ' + str(input_from_lower_level_number_of_channels_Decoder2))
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_cross_connection_channels=self.down1.final_number_of_channels + 2*(int(running_average_input/16)) + 2*self.number_of_channels_LSTM_CCL_to_decoder, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 3], [1, 2, 3], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [True,True,True],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(2);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_running_average_output_previous = None
        self.center_regular_output_previous_to_encoder = None
        self.center_regular_output_previous_to_decoder = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(unshuffled_input1);

        # ### Previous RNN Outputs: ###
        # self.center_running_average_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_regular_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            self.center_running_average_output_previous = unshuffled_input2;
            self.center_regular_output_previous_to_encoder = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_regular_output_previous_to_decoder = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
        if reset_flags_list[0] == 3:
            self.center_regular_output_previous_to_encoder = self.center_regular_output_previous_to_encoder.detach()
            self.center_regular_output_previous_to_decoder = self.center_regular_output_previous_to_decoder.detach()
            self.center_running_average_output_previous = self.center_running_average_output_previous.detach()


        ### Shuffle / Upample Previous RNN Outputs: ##
        center_running_average_output_previous_shuffled1 = self.shuffle1(self.center_running_average_output_previous)
        center_running_average_output_previous_shuffled2 = self.shuffle2(self.center_running_average_output_previous)
        center_regular_output_previous_to_encoder_upsampled1 = self.upsample1(self.center_regular_output_previous_to_encoder)
        center_regular_output_previous_to_encoder_upsampled2 = self.upsample2(self.center_regular_output_previous_to_encoder)
        center_regular_output_previous_to_decoder_upsampled1 = self.upsample1(self.center_regular_output_previous_to_decoder)
        center_regular_output_previous_to_decoder_upsampled2 = self.upsample2(self.center_regular_output_previous_to_decoder)


        # (1). Encoder:
        # print('x = ' + str(x.shape))
        # print('center_running_average_output_previous_shuffled1 = ' + str(center_running_average_output_previous_shuffled1.shape))
        # print('center_regular_output_previous_to_encoder_upsampled1 = ' + str(center_regular_output_previous_to_encoder_upsampled1.shape))
        # print('center_running_average_output_previous_shuffled2 = ' + str(center_running_average_output_previous_shuffled2.shape))
        # print('center_regular_output_previous_to_encoder_upsampled2 = ' + str(center_regular_output_previous_to_encoder_upsampled2.shape))
        #   (1.1). Encoder1:
        encoder1_input = torch.cat([x,center_running_average_output_previous_shuffled2], dim=1)  #regular input = raw RGB input + running_average
        encoder1_outside_connection = torch.cat([center_regular_output_previous_to_encoder_upsampled2], dim=1);  #outside connection = lstm_output
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)
        #   (1.2). Encoder2:
        encoder2_input = torch.cat([maxpool1,center_running_average_output_previous_shuffled1,], dim=1)   #regular input = encoder1 output + running_average
        encoder2_outside_connection = torch.cat([unshuffled_input1, center_regular_output_previous_to_encoder_upsampled1], dim=1) #outside_connection = raw unshuffled RGB input + lstm_output
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        # (2). Center:
        #   (2.2). Current RNN Outputs:
        #TODO: combine all outputs in network definition and not in-loop as it tends to create clutter
        #TODO: insert running average into LSTM i think...i image it can use it
        center_running_average_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_regular_output = self.center(maxpool2, reset_flags_list)
        center_regular_output_running_average = self.center_running_average_of_lstm_output(center_regular_output)
        center_regular_output_alpha_estimator = self.center_alpha_estimator_of_lstm_output(center_regular_output)
        center_regular_output_running_sum = self.center_running_sum_of_lstm_output(center_regular_output)
        combined_center_regular_output = torch.cat([center_regular_output, center_regular_output_running_average, center_regular_output_alpha_estimator, center_regular_output_running_sum], dim=1)
        #   (2.3). Color-Conversion-Layer for lstm output to reduce large number of channels:
        combined_center_regular_output_to_decoder = self.Color_Conversion_Layer_center_to_decoder(combined_center_regular_output)
        combined_center_regular_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_regular_output)
        #   (2.4). Assign previous outputs for NEXT TIME:
        self.center_running_average_output_previous = center_running_average_output
        self.center_regular_output_previous_to_encoder = combined_center_regular_output_to_encoder_next_time
        self.center_regular_output_previous_to_decoder = combined_center_regular_output_to_decoder
        #   (2.5). Upsample Current RNN Outputs:
        center_running_average_output_shuffled1 = self.shuffle1(center_running_average_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_running_average_output_shuffled2 = self.shuffle2(center_running_average_output)
        center_regular_output_upsampled1 = self.upsample1(combined_center_regular_output)
        center_regular_output_upsampled2 = self.upsample2(combined_center_regular_output)


        # (3). Decoder:
        decoder2_lstm_inputs = torch.cat([center_regular_output_previous_to_decoder_upsampled1], dim=1)
        decoder2_running_average_inputs = torch.cat([center_running_average_output_shuffled1, center_running_average_output_previous_shuffled1], dim=1)
        decoder2_direct_input = torch.cat([combined_center_regular_output_to_decoder], dim=1)
        decoder2_cross_connection = torch.cat([decoder2_running_average_inputs, decoder2_lstm_inputs, encoder2], dim=1)
        # print(decoder2_cross_connection.shape)
        # print(decoder2_direct_input.shape)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)

        # print(center_running_average_output_shuffled2.shape)
        # print(center_regular_output_upsampled2.shape)
        # print(center_running_average_output_previous_shuffled2.shape)
        # print(center_regular_output_previous_to_encoder_upsampled2.shape)
        # print(encoder1.shape)
        # print(decoder2.shape)
        # print(decoder1_cross_connection.shape)

        decoder1_lstm_inputs = torch.cat([center_regular_output_upsampled2, center_regular_output_previous_to_decoder_upsampled2], dim=1)
        decoder1_running_average_inputs = torch.cat([center_running_average_output_shuffled2, center_running_average_output_previous_shuffled2], dim=1)
        decoder1_cross_connection = torch.cat([decoder1_running_average_inputs, decoder1_lstm_inputs, encoder1], dim=1)
        decoder1_direct_input = torch.cat([decoder2], dim=1)
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################




























######################################################################################################################################################################################################################################################################################################################
class UNET_V26_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V26_V1, self).__init__()
        #(*). ENTER DEFORMABLE CONVOLUTIONS! + INPUT INTENSITY EXPLICITELY!

        # Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        # Pre-Encoder:
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ###########################################################################################################################################################
        # Encoder Levels:
        # (1). 3->32
        final_RGB_previous_number_of_channels = 3
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;

        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        # encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=Initial_CCL_number_of_output_channels + final_RGB_previous_number_of_channels + RGB_input_RunningAverage_number_of_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=self.number_of_channels_LSTM_CCL_to_encoder_next_time,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=True,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse']) # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'


        # (2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]

        #   (1.2). Encoder2:
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        # encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2 + number_of_channels_from_running_average_ouside_input_Encoder2 + 4*RGB_input_RunningAverage_number_of_channels, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=number_of_channels_from_running_average_ouside_input_Encoder2 + self.number_of_channels_LSTM_CCL_to_encoder_next_time,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=True,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'])  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'

        ###########################################################################################################################################################




        ###########################################################################################################################################################
        # Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        # TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center

        ### Number of channels: ###
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)

        #(1). raw unshuffled RGB convmean2d output:
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False)
        # (2). different output activations of regular LSTM-Flags output:
        #   (2.1). LSTM_flags:
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center + 2*(running_average_input),  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, False])




        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        # self.channels_from_center = 384;
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        # self.center_modules_list = nn.ModuleList([self.center_RGB_RunningAverage, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################




        ###########################################################################################################################################################
        # Decoder Levels:
        # (1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]

        # final_RGB_previous_number_of_channels = 3
        # RGB_input_RunningAverage_number_of_channels = 3;
        # RGB_input_number_of_channels = 3
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)

        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.number_of_channels_LSTM_CCL_to_decoder,
                                             number_of_lower_level_channels_after_upsample=self.number_of_channels_LSTM_CCL_to_decoder,
                                             number_of_cross_connection_channels=self.down2.final_number_of_channels + 1*int(running_average_input/4) + self.number_of_channels_LSTM_CCL_to_decoder + 4*RGB_input_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'])  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
        # (2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        # print('down1 = ' + str(self.down1.final_number_of_channels))
        # print('running average input * 2 = ' + str(2*(int(running_average_input/16))))
        # print('center output size * 2 = ' + str(2*self.center_output_size))
        # print('input cross connection = ' + str(self.down1.final_number_of_channels + 2*(int(running_average_input/16) + 2*self.center_output_size)))
        # print('input from lower level = ' + str(input_from_lower_level_number_of_channels_Decoder2))

        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_cross_connection_channels=self.down1.final_number_of_channels + 1*(int(running_average_input/16)) + 2*self.number_of_channels_LSTM_CCL_to_decoder + RGB_input_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'])  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_output_previous = self.final_output_previous.detach()


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        # (1). Encoder:
        # print('x = ' + str(x.shape))
        # print('center_RGB_RunningAverage_output_previous_shuffled1 = ' + str(center_RGB_RunningAverage_output_previous_shuffled1.shape))
        # print('center_RNN_output_to_encoder_previous_upsampled1 = ' + str(center_RNN_output_to_encoder_previous_upsampled1.shape))
        # print('center_RGB_RunningAverage_output_previous_shuffled2 = ' + str(center_RGB_RunningAverage_output_previous_shuffled2.shape))
        # print('center_RNN_output_to_encoder_previous_upsampled2 = ' + str(center_RNN_output_to_encoder_previous_upsampled2.shape))
        #   (1.1). Encoder1:
        encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)
        #   (1.2). Encoder2:
        # print('center_RNN_output_to_encoder_previous_upsampled1 = ' + str(center_RNN_output_to_encoder_previous_upsampled1.shape))
        # print('unshuffled_input1 = ' + str(unshuffled_input1.shape))
        # print('maxpool1 = ' + str(maxpool1.shape))
        # print('final_RGB_output_previous_unshuffled1 = ' + str(final_RGB_output_previous_unshuffled1.shape))
        # print('center_RGB_RunningAverage_output_previous_shuffled1 = ' + str(center_RGB_RunningAverage_output_previous_shuffled1.shape))
        encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)   #regular input = encoder1 output + running_average
        encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1) #outside_connection = raw unshuffled RGB input + lstm_output
        # print('encoder2_input = ' + str(encoder2_input.shape))
        # print('encoder2_outside_connection = ' + str(encoder2_outside_connection.shape))
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        # (2). Center:
        #   (2.1). Current RNN Outputs:
        #TODO: combine all outputs in network definition and not in-loop as it tends to create clutter
        #TODO: insert running average into LSTM i think...i image it can use it
        #       (2.1.1). Running Average (RA):
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        # (3). Decoder:
        #   (3.1). Decoder2:
        decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  #TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  #Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # print(decoder2_cross_connection.shape)
        # print(decoder2_direct_input.shape)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!

        # print(center_RGB_RunningAverage_output_shuffled2.shape)
        # print(center_RNN_output_to_decoder_upsampled2.shape)
        # print(center_RGB_RunningAverage_output_previous_shuffled2.shape)
        # print(center_RNN_output_to_encoder_previous_upsampled2.shape)
        # print(encoder1.shape)
        # print(decoder2.shape)
        # print(decoder1_cross_connection.shape)
        #   (3.2). Decoder1:
        decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1) #TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        decoder1_previous_final_RGB_input = final_RGB_output_previous
        decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        decoder1_direct_input = torch.cat([decoder2], dim=1)
        # print('decoder1_direct_input ' + str(decoder1_direct_input.shape))
        # print('decoder1_cross_connection ' + str(decoder1_cross_connection.shape))
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)

        ### Assign Final Output (Detached for now!!!): ###
        self.final_output_previous = decoder1.detach()

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


























######################################################################################################################################################################################################################################################################################################################
class UNET_V27_V1(nn.Module):
    def __init__(self, memory_unit_name='gru'):
        super(UNET_V27_V1, self).__init__()
        #(*). ENTER DEFORMABLE CONVOLUTIONS! + INPUT INTENSITY EXPLICITELY!

        # Parameters:
        activation_function = 'leakyrelu';
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        # Pre-Encoder:
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ###########################################################################################################################################################
        # Encoder Levels:
        # (1). 3->32
        final_RGB_previous_number_of_channels = 3
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;

        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        # encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=Initial_CCL_number_of_output_channels + RGB_input_RunningAverage_number_of_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=self.number_of_channels_LSTM_CCL_to_encoder_next_time,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=True,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse']) # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'


        # (2). 32->64
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]

        #   (1.2). Encoder2:
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        # encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=input_from_upper_level_number_of_channels_Encoder2 + number_of_channels_from_running_average_ouside_input_Encoder2, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=number_of_channels_from_running_average_ouside_input_Encoder2 + self.number_of_channels_LSTM_CCL_to_encoder_next_time,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=True,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'])  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'

        ###########################################################################################################################################################




        ###########################################################################################################################################################
        # Encoder/Decoder Center:  128->128
        # input_from_upper_level_number_of_channels_Center = output_number_of_channels_Encoder2
        # TODO: maybe insert the unshuffled input to the self.center as well?....maybe later
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center

        ### Number of channels: ###
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)

        #(1). raw unshuffled RGB convmean2d output:
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=running_average_input,
                                                      hidden_sizes=running_average_input,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False)
        # (2). different output activations of regular LSTM-Flags output:
        #   (2.1). LSTM_flags:
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=input_from_upper_level_number_of_channels_Center,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[True, True, True],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, False])




        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        # self.channels_from_center = 384;
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        # self.center_modules_list = nn.ModuleList([self.center_RGB_RunningAverage, self.center])
        # self.center_combined = Concat_Block(self.center_modules_list)
        ############################################################################################################################################################




        ###########################################################################################################################################################
        # Decoder Levels:
        # (1). 128->64
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]

        # final_RGB_previous_number_of_channels = 3
        # RGB_input_RunningAverage_number_of_channels = 3;
        # RGB_input_number_of_channels = 3
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)

        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.number_of_channels_LSTM_CCL_to_decoder,
                                             number_of_lower_level_channels_after_upsample=self.number_of_channels_LSTM_CCL_to_decoder,
                                             number_of_cross_connection_channels=self.down2.final_number_of_channels + 1*int(running_average_input/4),  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[True, True, True],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'])  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
        # (2). 64->32
        # input_from_lower_level_number_of_channels_Decoder2 = output_number_of_channels_Decoder1
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        output_number_of_channels_Decoder2 = hidden_sizes_Decoder2[-1]
        # print('down1 = ' + str(self.down1.final_number_of_channels))
        # print('running average input * 2 = ' + str(2*(int(running_average_input/16))))
        # print('center output size * 2 = ' + str(2*self.center_output_size))
        # print('input cross connection = ' + str(self.down1.final_number_of_channels + 2*(int(running_average_input/16) + 2*self.center_output_size)))
        # print('input from lower level = ' + str(input_from_lower_level_number_of_channels_Decoder2))

        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_lower_level_channels_after_upsample=input_from_lower_level_number_of_channels_Decoder2,
                                             number_of_cross_connection_channels=self.down1.final_number_of_channels + 1*(int(running_average_input/16)) + 1*self.number_of_channels_LSTM_CCL_to_decoder, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=True,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [True,True,True],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'])  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_output_previous = self.final_output_previous.detach()


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        # (1). Encoder:
        # print('x = ' + str(x.shape))
        # print('center_RGB_RunningAverage_output_previous_shuffled1 = ' + str(center_RGB_RunningAverage_output_previous_shuffled1.shape))
        # print('center_RNN_output_to_encoder_previous_upsampled1 = ' + str(center_RNN_output_to_encoder_previous_upsampled1.shape))
        # print('center_RGB_RunningAverage_output_previous_shuffled2 = ' + str(center_RGB_RunningAverage_output_previous_shuffled2.shape))
        # print('center_RNN_output_to_encoder_previous_upsampled2 = ' + str(center_RNN_output_to_encoder_previous_upsampled2.shape))
        #   (1.1). Encoder1:
        encoder1_input = torch.cat([x, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)
        #   (1.2). Encoder2:
        # print('center_RNN_output_to_encoder_previous_upsampled1 = ' + str(center_RNN_output_to_encoder_previous_upsampled1.shape))
        # print('unshuffled_input1 = ' + str(unshuffled_input1.shape))
        # print('maxpool1 = ' + str(maxpool1.shape))
        # print('final_RGB_output_previous_unshuffled1 = ' + str(final_RGB_output_previous_unshuffled1.shape))
        # print('center_RGB_RunningAverage_output_previous_shuffled1 = ' + str(center_RGB_RunningAverage_output_previous_shuffled1.shape))
        encoder2_input = torch.cat([maxpool1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)   #regular input = encoder1 output + running_average
        encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1) #outside_connection = raw unshuffled RGB input + lstm_output
        # print('encoder2_input = ' + str(encoder2_input.shape))
        # print('encoder2_outside_connection = ' + str(encoder2_outside_connection.shape))
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        # (2). Center:
        #   (2.1). Current RNN Outputs:
        #TODO: combine all outputs in network definition and not in-loop as it tends to create clutter
        #TODO: insert running average into LSTM i think...i image it can use it
        #       (2.1.1). Running Average (RA):
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = torch.cat([maxpool2], dim=1)
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        # (3). Decoder:
        #   (3.1). Decoder2:
        decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  #TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs], dim=1)  #Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # print(decoder2_cross_connection.shape)
        # print(decoder2_direct_input.shape)
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!

        # print(center_RGB_RunningAverage_output_shuffled2.shape)
        # print(center_RNN_output_to_decoder_upsampled2.shape)
        # print(center_RGB_RunningAverage_output_previous_shuffled2.shape)
        # print(center_RNN_output_to_encoder_previous_upsampled2.shape)
        # print(encoder1.shape)
        # print(decoder2.shape)
        # print(decoder1_cross_connection.shape)
        #   (3.2). Decoder1:
        decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2], dim=1)
        decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1) #TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        decoder1_previous_final_RGB_input = final_RGB_output_previous
        decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs], dim=1)
        decoder1_direct_input = torch.cat([decoder2], dim=1)
        # print('decoder1_direct_input ' + str(decoder1_direct_input.shape))
        # print('decoder1_cross_connection ' + str(decoder1_cross_connection.shape))
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)

        ### Assign Final Output (Detached for now!!!): ###
        self.final_output_previous = decoder1.detach()

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

















######################################################################################################################################################################################################################################################################################################################
class UNET_V28_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V28_V1, self).__init__()
        #(*). ENTER DEFORMABLE CONVOLUTIONS!
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        # Pre-Encoder:
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)



        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = True
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += RGB_input_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = True
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = True
        self.flag_center_RNN_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = False
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = self.down2.final_number_of_channels;
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = True
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = False
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = self.down1.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)



        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = encoder2;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = encoder1
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


























######################################################################################################################################################################################################################################################################################################################
class UNET_V29_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V29_V1, self).__init__()
        #(*). Explicit use of optical flow using TVNet!!!!
        #(*). ENTER DEFORMABLE CONVOLUTIONS!
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = True
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += RGB_input_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = True
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = True
        self.flag_center_RNN_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = False
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = self.down2.final_number_of_channels;
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = True
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = False
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = self.down1.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            ### Optical Flow Related - USE TVNET!!!!: ###
            self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            self.delta_x = self.delta_x.detach()
            self.delta_y = self.delta_y.detach()
            self.final_RGB_output_previous_warped = x2_warped


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)



        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = encoder2;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = encoder1
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################






















######################################################################################################################################################################################################################################################################################################################
class UNET_V30_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V30_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). ENTER DEFORMABLE CONVOLUTIONS!
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        # if self.flag_input_raw_intensity_instead_of_rgb:
        #     final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = self.down2.final_number_of_channels;
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = self.down1.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            # if self.flag_input_raw_intensity_instead_of_rgb:
            #     self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            # if self.flag_input_raw_intensity_instead_of_rgb:
            #     self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)



        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = encoder2;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = encoder1
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
















######################################################################################################################################################################################################################################################################################################################
class UNET_V31_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V31_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). ENTER DEFORMABLE CONVOLUTIONS!
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = self.down2.final_number_of_channels;
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = self.down1.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)



        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = encoder2;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = encoder1
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################











######################################################################################################################################################################################################################################################################################################################
class UNET_V32_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V32_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)



        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



















######################################################################################################################################################################################################################################################################################################################
class UNET_V33_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V33_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        ###
        #(*). Try to use 1 Extra Channel per layer which calculates Optical flow with previous frame intensity and outputs offsets for previous frame to be warped
        ###
        #(*). TODO: try the same weights/kernels for each scale when using the multiple_kernels block (with multiple dilations)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?

        ### Sequential Warping: ###
        self.optical_flow_dedicated_number_of_channels = 1; #Only intensity
        self.optical_flow_confidence_number_of_channels = 1;
        self.optical_flow_correspondence_number_of_channels = 1; #T-map / update_gate ?

        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels, 3 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels, 3 * 4 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels, 3 * 4 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels , 3 * 4 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels, 3 * 4 * channel_factor + self.optical_flow_dedicated_number_of_channels, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped


        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)



        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
















######################################################################################################################################################################################################################################################################################################################
class UNET_V34_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V34_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=16,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3')


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Get_Memory_Unit(memory_unit_name=memory_unit_name,
                                      input_size=self.center_RNN_number_of_input_channels,  # 2 is the number of layers
                                      hidden_sizes=hidden_sizes_Center,
                                      n_layers=len(hidden_sizes_Center),
                                      kernel_sizes=[5, 5, 5], strides=1, dilations=1, groups=1,
                                      normalization_function=normalization_function,
                                      activation_function=activation_function,
                                      initialization_method='xavier',
                                      flag_deformable_convolution=[False, False, False],
                                      flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                      flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                      flag_deformable_convolution_modulation=[False, False, True],
                                      flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)
        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = True
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = True
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,PCD_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output)
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output)
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################














######################################################################################################################################################################################################################################################################################################################
class UNET_V35_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V35_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=16,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3')


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        flag_deformable_convolution=[False, False, False],
                                                        flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, True],
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        flag_deformable_for_each_sub_block_or_for_super_block='super_block',
                                                        flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,PCD_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


















######################################################################################################################################################################################################################################################################################################################
class UNET_V36_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V36_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). TVNet At Start!!!
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=16,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3')


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        flag_deformable_convolution=[False, False, False],
                                                        flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, True],
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        flag_deformable_for_each_sub_block_or_for_super_block='super_block',
                                                        flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=True)


        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            #############################
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()


        ### Optical Flow Related - USE TVNET!!!!: ###
        self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x.mean(dim=1,keepdim=True), self.final_RGB_output_previous)
        self.delta_x = self.delta_x.detach()
        self.delta_y = self.delta_y.detach()
        # self.final_RGB_output_previous_warped = x2_warped
        self.final_RGB_output_previous = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,PCD_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


















######################################################################################################################################################################################################################################################################################################################
class UNET_V37_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V37_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = True


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=16,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=2)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        flag_deformable_convolution=[False, False, False],
                                                        flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, True],
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        flag_deformable_for_each_sub_block_or_for_super_block='super_block',
                                                        flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,PCD_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
























######################################################################################################################################################################################################################################################################################################################
class UNET_V38_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V38_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). USE TSA - maybe even as conditional!
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=16,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=2)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=[1, 1], dilations=[[1, 2, 4], [1, 2, 4]], groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 flag_deformable_convolution=[False, False, False],
                                                 flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                                 flag_deformable_same_on_all_channels=[True,True,True] )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        flag_deformable_convolution=[False, False, False],
                                                        flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, True],
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        flag_deformable_for_each_sub_block_or_for_super_block='super_block',
                                                        flag_deformable_same_on_all_channels=[True, True, True])

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             flag_deformable_convolution=[False, False, False],
                                             flag_deformable_convolution_version=['v1', 'v1', 'v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],
                                             flag_deformable_convolution_modulation=[False, False, True],
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             flag_deformable_convolution = [False,False,False],
                                             flag_deformable_convolution_version = ['v1','v1','v1'],
                                             flag_deformable_convolution_before_or_after_main_convolution = ['before','before','before'],
                                             flag_deformable_convolution_modulation = [False,False,False],
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             flag_deformable_for_each_sub_block_or_for_super_block='super_block',  # 'super_block' / 'sub_block'
                                             flag_deformable_same_on_all_channels=[True,True,True])

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,PCD_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input) #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################














######################################################################################################################################################################################################################################################################################################################
class UNET_V38_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V38_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=16,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=2)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!


        ### Prepare y_cell_conditional_input for different places on the Network: ###
        y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,PCD_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
























######################################################################################################################################################################################################################################################################################################################
class UNET_V39_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V39_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_number_of_feature_channels = 16
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=self.PCD_number_of_feature_channels,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=2)

        ### TSA Module: ###
        reference_reference_frame_index = 1;
        number_of_frames_compared = 2; #only current and previous...maybe later on i can save several Intensity Only backward frames
        self.TSA_Fusion_Module = TSA_Fusion(nf=self.PCD_number_of_output_channels, nframes=number_of_frames_compared, center=reference_reference_frame_index)


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);

        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)


        ### PCD Feature Extraction and Alignment Module: ###
        x_intensity = x.mean(dim=1, keepdim=True)
        PCD_output_reference_previous = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!
        PCD_output_reference_reference = self.PCD_Extract_and_Align_Module(x_intensity, x_intensity)

        ### TSA Fusion Module: ###
        #(1). Combine PCD_reference_previous & PCD_reference_reference to get TSA_Fusion_Module inputs which does Spatio-Temporal Weighing & Mixture of Features:
        TSA_input = torch.cat([PCD_output_reference_previous.unsqueeze(1),PCD_output_reference_reference.unsqueeze(1)],dim=1)  #[B,C,H,W] -> [B,N,C,H,W]
        #(2). TSA Module:
        TSA_output = self.TSA_Fusion_Module(TSA_input)


        #############################################################################
        ### Prepare y_cell_conditional_input for different places on the Network: ###
        y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)
        #############################################################################


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,TSA_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
































######################################################################################################################################################################################################################################################################################################################
class UNET_V40_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V40_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Using Sequential_Conv_Block on y_cell_conditional before inserting them as conditionals - TODO: decide whether to active that small Network on each RGB seperately or together
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_number_of_feature_channels = 16
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=self.PCD_number_of_feature_channels,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=2)

        ### TSA Module: ###
        reference_reference_frame_index = 1;
        number_of_frames_compared = 2; #only current and previous...maybe later on i can save several Intensity Only backward frames
        self.TSA_Fusion_Module = TSA_Fusion(nf=self.PCD_number_of_output_channels, nframes=number_of_frames_compared, center=reference_reference_frame_index)


        ### SFT Conditionals Small Net: ###
        self.Conditionals_Network = Sequential_Conv_Block_General(
                                                        number_of_input_channels= 1,  #Only Intensity For Each Feature Seperately
                                                        number_of_output_channels=[3 * 4, 3 * 4, 3],
                                                        kernel_sizes=[[3], [3], [3]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[False, False, False],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None

        self.AvgPool2d = nn.AvgPool2d(2)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);


        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)
        #(3). DownSample Input and Previouse-Output RGB Using AvgPool2d:
        x_intensity = x.mean(dim=1, keepdim=True)
        x_intensity_downsampled1 = self.AvgPool2d(x_intensity)
        x_intensity_downsampled2 = self.AvgPool2d(x_intensity_downsampled1)
        final_RGB_output_previous_downsampled1 = self.AvgPool2d(final_RGB_output_previous)
        final_RGB_output_previous_downsampled2 = self.AvgPool2d(final_RGB_output_previous_downsampled1)

        ### PCD Feature Extraction and Alignment Module: ###
        PCD_output_reference_previous = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!
        PCD_output_reference_reference = self.PCD_Extract_and_Align_Module(x_intensity, x_intensity)

        ### TSA Fusion Module: ###
        #(1). Combine PCD_reference_previous & PCD_reference_reference to get TSA_Fusion_Module inputs which does Spatio-Temporal Weighing & Mixture of Features:
        TSA_input = torch.cat([PCD_output_reference_previous.unsqueeze(1),PCD_output_reference_reference.unsqueeze(1)],dim=1)  #[B,C,H,W] -> [B,N,C,H,W]
        #(2). TSA Module:
        TSA_output = self.TSA_Fusion_Module(TSA_input)


        #############################################################################
        # ### Conditional Network On Concatenated-Features(!): ###
        # y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        # y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        # y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)
        ### Conditional Network On Each Feature Seperately: ###
        y_cell_conditional_input_level1_1 = self.Conditionals_Network(x_intensity)
        y_cell_conditional_input_level2_1 = self.Conditionals_Network(x_intensity_downsampled1)
        y_cell_conditional_input_center_1 = self.Conditionals_Network(x_intensity_downsampled2)
        y_cell_conditional_input_level1_2 = self.Conditionals_Network(final_RGB_output_previous)
        y_cell_conditional_input_level2_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled1)
        y_cell_conditional_input_center_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled2)
        y_cell_conditional_input_level1 = torch.cat([y_cell_conditional_input_level1_1, y_cell_conditional_input_level1_2], dim=1)
        y_cell_conditional_input_level2 = torch.cat([y_cell_conditional_input_level2_1, y_cell_conditional_input_level2_2], dim=1)
        y_cell_conditional_input_center = torch.cat([y_cell_conditional_input_center_1, y_cell_conditional_input_center_2], dim=1)
        #############################################################################


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,TSA_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################























######################################################################################################################################################################################################################################################################################################################
class UNET_V41_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V41_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Using Sequential_Conv_Block on y_cell_conditional before inserting them as conditionals - TODO: decide whether to active that small Network on each RGB seperately or together
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_number_of_feature_channels = 16
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=self.PCD_number_of_feature_channels,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=2)

        ### TSA Module: ###
        reference_reference_frame_index = 1;
        number_of_frames_compared = 2; #only current and previous...maybe later on i can save several Intensity Only backward frames
        self.TSA_Fusion_Module = TSA_Fusion(nf=self.PCD_number_of_output_channels, nframes=number_of_frames_compared, center=reference_reference_frame_index)


        ### SFT Conditionals Small Net: ###
        self.Conditionals_Network = Sequential_Conv_Block_General(
                                                        number_of_input_channels= 1,  #Only Intensity For Each Feature Seperately
                                                        number_of_output_channels=[3 * 4, 3 * 4, 3],
                                                        kernel_sizes=[[3], [3], [3]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[False, False, False],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1_cross_connection_projection = Color_Space_Conversion_Layer(12,3)
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer + self.PCD_number_of_output_channels, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2_cross_connection_projection = Color_Space_Conversion_Layer(12*4, 3)
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None

        self.AvgPool2d = nn.AvgPool2d(2)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);


        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)
        #(3). DownSample Input and Previouse-Output RGB Using AvgPool2d:
        x_intensity = x.mean(dim=1, keepdim=True)

        ### PCD Feature Extraction and Alignment Module: ###
        PCD_output_reference_previous = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!
        PCD_output_reference_reference = self.PCD_Extract_and_Align_Module(x_intensity, x_intensity)

        ### TSA Fusion Module: ###
        #(1). Combine PCD_reference_previous & PCD_reference_reference to get TSA_Fusion_Module inputs which does Spatio-Temporal Weighing & Mixture of Features:
        TSA_input = torch.cat([PCD_output_reference_previous.unsqueeze(1),PCD_output_reference_reference.unsqueeze(1)],dim=1)  #[B,C,H,W] -> [B,N,C,H,W]
        #(2). TSA Module:
        TSA_output = self.TSA_Fusion_Module(TSA_input)


        #############################################################################
        # ### Conditional Network On Concatenated-Features Input&Output RGB(!): ###
        # y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        # y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        # y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)
        # ### Conditional Network On Each Feature Seperately Input&Output RGB(!): ###
        # x_intensity_downsampled1 = self.AvgPool2d(x_intensity)
        # x_intensity_downsampled2 = self.AvgPool2d(x_intensity_downsampled1)
        # final_RGB_output_previous_downsampled1 = self.AvgPool2d(final_RGB_output_previous)
        # final_RGB_output_previous_downsampled2 = self.AvgPool2d(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_level1_1 = self.Conditionals_Network(x_intensity)
        # y_cell_conditional_input_level2_1 = self.Conditionals_Network(x_intensity_downsampled1)
        # y_cell_conditional_input_center_1 = self.Conditionals_Network(x_intensity_downsampled2)
        # y_cell_conditional_input_level1_2 = self.Conditionals_Network(final_RGB_output_previous)
        # y_cell_conditional_input_level2_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_center_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled2)
        # y_cell_conditional_input_level1 = torch.cat([y_cell_conditional_input_level1_1, y_cell_conditional_input_level1_2], dim=1)
        # y_cell_conditional_input_level2 = torch.cat([y_cell_conditional_input_level2_1, y_cell_conditional_input_level2_2], dim=1)
        # y_cell_conditional_input_center = torch.cat([y_cell_conditional_input_center_1, y_cell_conditional_input_center_2], dim=1)
        ### Use TSA Output As Conditionals: ###
        y_cell_conditional_input_level1 = torch.cat([TSA_output,x],dim=1);
        y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        #############################################################################


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input,TSA_output],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################



















######################################################################################################################################################################################################################################################################################################################
class UNET_V43_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V43_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Using Sequential_Conv_Block on y_cell_conditional before inserting them as conditionals - TODO: decide whether to active that small Network on each RGB seperately or together
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_number_of_feature_channels = 16
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=self.PCD_number_of_feature_channels,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=1)

        ### TSA Module: ###
        reference_reference_frame_index = 1;
        number_of_frames_compared = 2; #only current and previous...maybe later on i can save several Intensity Only backward frames
        self.TSA_Fusion_Module = TSA_Fusion(nf=self.PCD_number_of_output_channels, nframes=number_of_frames_compared, center=reference_reference_frame_index)


        ### SFT Conditionals Small Net: ###
        self.Conditionals_Network = Sequential_Conv_Block_General(
                                                        number_of_input_channels= self.PCD_number_of_output_channels * 2,  #Only Intensity For Each Feature Seperately
                                                        number_of_output_channels=[3, 3, 3],
                                                        kernel_sizes=[[3], [3], [3]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[False, False, False],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1_cross_connection_projection = Color_Space_Conversion_Layer(12,3)
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2_cross_connection_projection = Color_Space_Conversion_Layer(12*4, 3)
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None

        self.AvgPool2d = nn.AvgPool2d(2)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);


        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)
        #(3). DownSample Input and Previouse-Output RGB Using AvgPool2d:
        x_intensity = x.mean(dim=1, keepdim=True)

        ### PCD Feature Extraction and Alignment Module: ###
        PCD_output_reference_previous = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!
        PCD_output_reference_reference = self.PCD_Extract_and_Align_Module(x_intensity, x_intensity)

        # ### TSA Fusion Module: ###
        # #(1). Combine PCD_reference_previous & PCD_reference_reference to get TSA_Fusion_Module inputs which does Spatio-Temporal Weighing & Mixture of Features:
        # TSA_input = torch.cat([PCD_output_reference_previous.unsqueeze(1),PCD_output_reference_reference.unsqueeze(1)],dim=1)  #[B,C,H,W] -> [B,N,C,H,W]
        # #(2). TSA Module:
        # TSA_output = self.TSA_Fusion_Module(TSA_input)


        #############################################################################
        # ### Conditional Network On Concatenated-Features Input&Output RGB(!): ###
        # y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        # y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        # y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)
        # ### Conditional Network On Each Feature Seperately Input&Output RGB(!): ###
        # x_intensity_downsampled1 = self.AvgPool2d(x_intensity)
        # x_intensity_downsampled2 = self.AvgPool2d(x_intensity_downsampled1)
        # final_RGB_output_previous_downsampled1 = self.AvgPool2d(final_RGB_output_previous)
        # final_RGB_output_previous_downsampled2 = self.AvgPool2d(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_level1_1 = self.Conditionals_Network(x_intensity)
        # y_cell_conditional_input_level2_1 = self.Conditionals_Network(x_intensity_downsampled1)
        # y_cell_conditional_input_center_1 = self.Conditionals_Network(x_intensity_downsampled2)
        # y_cell_conditional_input_level1_2 = self.Conditionals_Network(final_RGB_output_previous)
        # y_cell_conditional_input_level2_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_center_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled2)
        # y_cell_conditional_input_level1 = torch.cat([y_cell_conditional_input_level1_1, y_cell_conditional_input_level1_2], dim=1)
        # y_cell_conditional_input_level2 = torch.cat([y_cell_conditional_input_level2_1, y_cell_conditional_input_level2_2], dim=1)
        # y_cell_conditional_input_center = torch.cat([y_cell_conditional_input_center_1, y_cell_conditional_input_center_2], dim=1)
        # ### Use TSA Output As Conditionals: ###
        # y_cell_conditional_input_level1 = torch.cat([TSA_output,x],dim=1);
        # y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        # y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        ### Use PCD Outputs As Conditionals: ###
        y_cell_conditional_input_level1 = torch.cat([PCD_output_reference_previous, PCD_output_reference_reference], dim=1);
        y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        #############################################################################


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################


























######################################################################################################################################################################################################################################################################################################################
class UNET_V44_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V44_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Using Sequential_Conv_Block on y_cell_conditional before inserting them as conditionals - TODO: decide whether to active that small Network on each RGB seperately or together
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_number_of_feature_channels = 16
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=self.PCD_number_of_feature_channels,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=1)

        ### TSA Module: ###
        reference_reference_frame_index = 1;
        number_of_frames_compared = 2; #only current and previous...maybe later on i can save several Intensity Only backward frames
        self.TSA_Fusion_Module = TSA_Fusion(nf=self.PCD_number_of_output_channels, nframes=number_of_frames_compared, center=reference_reference_frame_index)


        ### SFT Conditionals Small Net: ###
        self.Conditionals_Network = Sequential_Conv_Block_General(
                                                        number_of_input_channels= 1 * 2,  #Only Intensity For Each Feature Seperately
                                                        number_of_output_channels=[3, 3, 3],
                                                        kernel_sizes=[[3], [3], [3]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[False, False, False],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1_cross_connection_projection = Color_Space_Conversion_Layer(12,3)
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = False
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2_cross_connection_projection = Color_Space_Conversion_Layer(12*4, 3)
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = False
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = False #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = False
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None

        self.AvgPool2d = nn.AvgPool2d(2)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=True)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);


        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)
        #(3). DownSample Input and Previouse-Output RGB Using AvgPool2d:
        x_intensity = x.mean(dim=1, keepdim=True)


        ### TVNet Warping: ###
        self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x_intensity, self.final_RGB_output_previous)
        self.delta_x = self.delta_x.detach()
        self.delta_y = self.delta_y.detach()
        self.final_RGB_output_previous = x2_warped

        # ### PCD Feature Extraction and Alignment Module: ###
        # PCD_output_reference_previous = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!
        # PCD_output_reference_reference = self.PCD_Extract_and_Align_Module(x_intensity, x_intensity)

        # ### TSA Fusion Module: ###
        # #(1). Combine PCD_reference_previous & PCD_reference_reference to get TSA_Fusion_Module inputs which does Spatio-Temporal Weighing & Mixture of Features:
        # TSA_input = torch.cat([PCD_output_reference_previous.unsqueeze(1),PCD_output_reference_reference.unsqueeze(1)],dim=1)  #[B,C,H,W] -> [B,N,C,H,W]
        # #(2). TSA Module:
        # TSA_output = self.TSA_Fusion_Module(TSA_input)


        #############################################################################
        # ### Conditional Network On Concatenated-Features Input&Output RGB(!): ###
        # y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        # y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        # y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)
        # ### Conditional Network On Each Feature Seperately Input&Output RGB(!): ###
        # x_intensity_downsampled1 = self.AvgPool2d(x_intensity)
        # x_intensity_downsampled2 = self.AvgPool2d(x_intensity_downsampled1)
        # final_RGB_output_previous_downsampled1 = self.AvgPool2d(final_RGB_output_previous)
        # final_RGB_output_previous_downsampled2 = self.AvgPool2d(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_level1_1 = self.Conditionals_Network(x_intensity)
        # y_cell_conditional_input_level2_1 = self.Conditionals_Network(x_intensity_downsampled1)
        # y_cell_conditional_input_center_1 = self.Conditionals_Network(x_intensity_downsampled2)
        # y_cell_conditional_input_level1_2 = self.Conditionals_Network(final_RGB_output_previous)
        # y_cell_conditional_input_level2_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_center_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled2)
        # y_cell_conditional_input_level1 = torch.cat([y_cell_conditional_input_level1_1, y_cell_conditional_input_level1_2], dim=1)
        # y_cell_conditional_input_level2 = torch.cat([y_cell_conditional_input_level2_1, y_cell_conditional_input_level2_2], dim=1)
        # y_cell_conditional_input_center = torch.cat([y_cell_conditional_input_center_1, y_cell_conditional_input_center_2], dim=1)
        ### Conditional Network On Input & Warped-Previous-Output Intensity: ###
        y_cell_conditional_input_level1 = self.Conditionals_Network(torch.cat([x_intensity, self.final_RGB_output_previous],dim=1))
        y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        # ### Use TSA Output As Conditionals: ###
        # y_cell_conditional_input_level1 = torch.cat([TSA_output,x],dim=1);
        # y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        # y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        # ### Use PCD Outputs As Conditionals: ###
        # y_cell_conditional_input_level1 = torch.cat([PCD_output_reference_previous, PCD_output_reference_reference], dim=1);
        # y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        # y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        #############################################################################


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        # self.flag_down2_get_final_RGB_previous = True
        # self.flag_down2_get_center_RGB_RunningAverage_previous = True
        # self.flag_down2_get_RawInputExplicit = True
        # self.flag_down2_get_center_RNN_previous = True
        ###
        # encoder2_input = torch.cat([maxpool1, final_RGB_output_previous_unshuffled1, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)  # regular input = encoder1 output + running_average
        # encoder2_outside_connection = torch.cat([unshuffled_input1, center_RNN_output_to_encoder_previous_upsampled1], dim=1)  # outside_connection = raw unshuffled RGB input + lstm_output
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        # self.flag_up2_get_RGBRunningAverage_current = True
        # self.flag_up2_get_RGBRunningAverage_previous = False
        # self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_final_RGB_previous = True  # TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        # self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_center_RNN_previous = True
        # self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up2_get_RAW_RGB_current = False
        # self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder2_lstm_inputs = torch.cat([center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        # decoder2_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled1], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder2_previous_final_RGB_input = final_RGB_output_previous_unshuffled1
        # decoder2_direct_input = torch.cat([center_RNN_output_to_decoder_current], dim=1)
        # decoder2_cross_connection = torch.cat([encoder2, decoder2_running_average_inputs, decoder2_lstm_inputs, decoder2_previous_final_RGB_input], dim=1)  # Maybe also insert RAW RGB input directly from the start??....similar to the logic of dense block?
        # decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_direct_input)  # (*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ###
        # self.flag_up1_get_center_RNN_previous = True
        # self.flag_up1_get_center_RNN_current = True
        # self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_center_RGBRunningAverage_previous = False
        # self.flag_up1_get_center_RGBRunningAverage_current = True
        # self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_final_RGB_previous = True
        # self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        # self.flag_up1_get_RAW_RGB_current = False
        # self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ###
        # decoder1_lstm_inputs = torch.cat([center_RNN_output_to_decoder_upsampled2, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        # decoder1_running_average_inputs = torch.cat([center_RGB_RunningAverage_output_shuffled2], dim=1)  # TODO: wait what?....do i really need to input current & previous RGB_RunningAverage?!?!?!
        # decoder1_previous_final_RGB_input = final_RGB_output_previous
        # decoder1_cross_connection = torch.cat([encoder1, decoder1_running_average_inputs, decoder1_lstm_inputs, decoder1_previous_final_RGB_input], dim=1)
        # decoder1_direct_input = torch.cat([decoder2], dim=1)
        # decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_direct_input)
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################































######################################################################################################################################################################################################################################################################################################################
class UNET_V45_V1(nn.Module):
    def __init__(self, memory_unit_name='gru', activation_function_input='leakyrelu'):
        super(UNET_V45_V1, self).__init__()
        #(*). No Running Average!!!
        #(*). No Skip Connection!!! TODO: try skip connection with high quantization?
        #(*). No RNN at center!!! (Sequential_General Instead)
        #(*). New Sequential_General Layer with conditionals$#@%@
        #(*). TRY USING y_cell_conditional with INPUT RGB or previous output RGB
        #(*). Using Sequential_Conv_Block on y_cell_conditional before inserting them as conditionals - TODO: decide whether to active that small Network on each RGB seperately or together
        #(*). Use as conditionals possibilities: optical_flow, structure_tensor, PCD-Align features, TSA Spatio-Temporal Correlations, T-map from speciality layers like RA and alpha_estimator etc', simply input RGB and previous final RGB (maybe after feature extractor layers?....)
        #(*). Possible Activation Functions: 'relu', 'leakyrelu', 'double_relu', 'sigmoid', 'none', 'swish_torch', 'diode_torch', 'normal_torch',
        #                                    'normal_derivative_torch', 'normal_derivative_modified_torch', 'normal_modified_torch', 'diode_torch_PositivityByExp', 'diode_torch_PositivityByAbs', 'diode_torch_OnlyCentersLearned', 'learnable_amplitude'


        # Parameters:
        # activation_function = 'leakyrelu';
        activation_function = activation_function_input;
        normalization_function = None;
        final_activation_function = 'none'
        channel_factor = 1;

        ### Flags: ###
        self.flag_input_raw_intensity_instead_of_rgb = True;
        self.UNET_Residual = False


        ### Optical Flow Module: ###
        self.TVNet_number_of_iterations = 30;
        self.TVNet_number_of_pyramid_scales = 4;
        self.TVNet_layer = None
        self.delta_x = None
        self.delta_y = None
        self.confidence = None #==RA update gate?


        ### Pre-Encoder: ###
        Initial_CCL_number_of_output_channels = 3; # RGB + Intensity
        self.Color_Conversion_Layer = Color_Space_Conversion_Layer(3, Initial_CCL_number_of_output_channels)

        ### PCD_Extract_and_Align Module To Extract Features From Input_RGB and Previous_Clean_Final_RGB and align them!: ###
        #TODO: maybe later also add TSA Module!
        self.PCD_number_of_output_channels = 6;
        self.PCD_number_of_feature_channels = 16
        self.PCD_Extract_and_Align_Module = PCD_Extract_and_Align(number_of_input_channels=1, #accept intensity
                                                                  number_of_feature_channels=self.PCD_number_of_feature_channels,
                                                                  number_of_output_channels=self.PCD_number_of_output_channels,
                                                                  flag_deformable_convolution_type='v3',
                                                                  number_of_deformable_groups=1)

        ### TSA Module: ###
        reference_reference_frame_index = 1;
        number_of_frames_compared = 2; #only current and previous...maybe later on i can save several Intensity Only backward frames
        self.TSA_Fusion_Module = TSA_Fusion(nf=self.PCD_number_of_output_channels, nframes=number_of_frames_compared, center=reference_reference_frame_index)


        ### SFT Conditionals Small Net: ###
        self.Conditionals_Network = Sequential_Conv_Block_General(
                                                        number_of_input_channels= self.PCD_number_of_output_channels * 2,  #Only Intensity For Each Feature Seperately
                                                        number_of_output_channels=[3, 3, 3],
                                                        kernel_sizes=[[3], [3], [3]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[False, False, False],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['x', 'x', 'x'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )


        ###########################################################################################################################################################
        #### Encoder1: ####
        ### Number of Channels: ###
        final_RGB_previous_number_of_channels = 3
        if self.flag_input_raw_intensity_instead_of_rgb:
            final_RGB_previous_number_of_channels = 1;
        RGB_input_RunningAverage_number_of_channels = 3;
        RGB_input_number_of_channels = 3
        self.number_of_channels_LSTM_CCL_to_encoder_next_time = 10;
        number_of_channels_Encoder1 = [3 * 4 * channel_factor, 3 * 4 * channel_factor]
        ### number of channels temp variables: ###
        input_from_upper_level_number_of_channels_Encoder1 = Initial_CCL_number_of_output_channels;
        output_number_of_channels_Encoder1 = number_of_channels_Encoder1[-1]
        number_of_channels_from_running_average_ouside_input_Encoder1 = Initial_CCL_number_of_output_channels;
        ### Flags: ###
        #TODO: divide to direct_input & outside_connections
        self.flag_down1_use_outside_connection = True;
        self.flag_down1_get_final_RGB_previous = True;
        self.flag_down1_get_center_RGB_RunningAverage_previous = False
        self.flag_down1_get_center_RNN_previous = True;
        ### Allocate number of channels: ###
        # (1). From Upper Level:
        down1_number_of_input_channels_from_upper_layer = RGB_input_number_of_channels;
        # (2). From Outside:
        down1_number_of_input_channels_from_outside = 0;
        if self.flag_down1_get_final_RGB_previous:
            down1_number_of_input_channels_from_outside += final_RGB_previous_number_of_channels
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            down1_number_of_input_channels_from_outside += RGB_input_RunningAverage_number_of_channels
        if self.flag_down1_get_center_RNN_previous:
            down1_number_of_input_channels_from_outside += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        self.down1_number_of_input_channels_from_outside = down1_number_of_input_channels_from_outside
        ### Define down1: ###
        self.down1_cross_connection_projection = Color_Space_Conversion_Layer(12,3)
        self.down1 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down1_number_of_input_channels_from_upper_layer, #raw input + raw_input_running_average
                                                 number_of_input_channels_from_cross_connection=down1_number_of_input_channels_from_outside,  #channels_from_center
                                                 number_of_output_channels=number_of_channels_Encoder1,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=number_of_channels_Encoder1[-1],
                                                 flag_use_cross_connection=self.flag_down1_use_outside_connection,  #TODO: for now i'm not using previous RNN & RA center outputs!!!$@#$
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 stack_residual_scale=1,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1,1,1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',  # DPP_learned
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y','y','y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5,5,5],
                                                 flag_deformable_number_of_deformable_groups=[-1,-1,-1],
                                                 flag_deformable_number_of_channels_in_group1=[-1,-1,-1],
                                                 flag_deformable_same_on_all_channels=[True,True,True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x','x','x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block','super_block','super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type = ['simple','simple'], # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type = ['131_collapse','131_collapse'],# 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )



        #### Encoder2: ####
        # input_from_upper_level_number_of_channels_Encoder2 = output_number_of_channels_Encoder1
        input_from_upper_level_number_of_channels_Encoder2 = self.down1.final_number_of_channels
        hidden_sizes_Encoder2 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]  # final element = output number of channels from layer
        output_number_of_channels_Encoder2 = hidden_sizes_Encoder2[-1]
        number_of_channels_from_running_average_ouside_input_Encoder2 = Initial_CCL_number_of_output_channels*4  #the X4 is because of the shuffle
        ### Flags: ###
        self.flag_down2_use_outside_connection = True
        self.flag_down2_get_final_RGB_previous = True
        self.flag_down2_get_center_RGB_RunningAverage_previous = False
        self.flag_down2_get_RawInputExplicit = True
        self.flag_down2_get_center_RNN_previous = True
        ### Allocate number of channels: ###
        #(1). From Upper Level:
        down2_number_of_input_channels_from_upper_layer = input_from_upper_level_number_of_channels_Encoder2;
        #(2). From Outside:
        down2_number_of_input_channels_from_outide = 0;
        if self.flag_down2_get_center_RNN_previous:
            down2_number_of_input_channels_from_outide += self.number_of_channels_LSTM_CCL_to_encoder_next_time
        if self.flag_down2_get_final_RGB_previous:
            down2_number_of_input_channels_from_outide += final_RGB_previous_number_of_channels * 4
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            down2_number_of_input_channels_from_outide += RGB_input_RunningAverage_number_of_channels * 4
        if self.flag_down2_get_RawInputExplicit:
            down2_number_of_input_channels_from_outide += RGB_input_number_of_channels * 4
        self.down2_number_of_input_channels_from_outide = down2_number_of_input_channels_from_outide
        self.down2_cross_connection_projection = Color_Space_Conversion_Layer(12*4, 3)
        self.down2 = UNET_Down_Fusion_General_V1(number_of_input_channels_from_upper_layer=down2_number_of_input_channels_from_upper_layer, #input_from_above + channels_from_center + raw_input_running_average_shuffled
                                                 number_of_input_channels_from_cross_connection=down2_number_of_input_channels_from_outide,  #TODO: maybe use a difference CCL for the encoder2
                                                 number_of_output_channels=hidden_sizes_Encoder2,
                                                 flag_use_final_projection_block=False,
                                                 number_of_output_channels_after_projection_block=hidden_sizes_Encoder2[-1],
                                                 flag_use_cross_connection=self.flag_down2_use_outside_connection,  # I should really call it something like "outer connection" or "outer information" because it's not a cross connection from another layer really or necessarily
                                                 flag_Sequential_or_RDB='sequential',
                                                 flag_sequential_dense=False,
                                                 flag_sequential_resnet=True,
                                                 flag_sequential_concat=False,
                                                 kernel_sizes=[[3, 3, 3], [3, 3, 3]],
                                                 strides=[1, 1],
                                                 dilations=[[1, 2, 4], [1, 2, 4]],
                                                 groups=[1, 1],
                                                 normalization_function=normalization_function,
                                                 activation_function=activation_function,
                                                 flag_downsample_strategy='DPP',
                                                 initialization_method='xavier',
                                                 flag_add_unshuffled_input_to_lower_level=False,
                                                 ##############################################   --  1-K-1 --(*+)---
                                                 ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                 flag_SuperBlock_SFT=[True, True, True],
                                                 flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                 flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                 flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                 flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                 flag_SuperBlock_SFT_shift=[True, True, True],
                                                 flag_SuperBlock_SFT_scale=[True, True, True],
                                                 ### Deformable Convolution: ###
                                                 flag_deformable_convolution=[False, False, True],
                                                 flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                 flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                 flag_deformable_convolution_modulation=[False, False, False],
                                                 flag_deformable_kernel_size=[5, 5, 5],
                                                 flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                 flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                 flag_deformable_same_on_all_channels=[True, True, True],
                                                 ### Deformable SFT: ###
                                                 flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                 flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                 flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                                 flag_deformable_SFT_shift=[False, False, False],
                                                 flag_deformable_SFT_scale=[False, False, False],
                                                 flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                 flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                 #####################################
                                                 flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                                 flag_super_cell_block_type=['131_collapse', '131_collapse'],  # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                                 )

        ###########################################################################################################################################################





        ###########################################################################################################################################################
        #### raw unshuffled RGB convmean2d RunningAverage output: ###
        ### Flags: ###
        self.flag_center_RGBRunningAverage_get_final_RGB_previous = False
        self.flag_center_RGBRunningAverage_get_RAW_RGB_current = False
        ### Allocate number of channels: ###
        self.center_RGB_RunningAverage_input_channels = RGB_input_number_of_channels * (4**2);
        self.center_RGB_RunningAverage_outside_number_of_channels = 0;
        if self.flag_center_RGBRunningAverage_get_final_RGB_previous:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        if self.flag_center_RGBRunningAverage_get_RAW_RGB_current:
            self.center_RGB_RunningAverage_outside_number_of_channels += RGB_input_number_of_channels * (4**2)
        self.center_RGB_unshuffled_running_average = Get_Memory_Unit(memory_unit_name='convmean2d',
                                                      input_size=self.center_RGB_RunningAverage_input_channels,
                                                      hidden_sizes=self.center_RGB_RunningAverage_input_channels,
                                                      n_layers=1,
                                                      kernel_sizes=[5], strides=1, dilations=1, groups=1,
                                                      normalization_function='none',
                                                      activation_function='none',
                                                      initialization_method='xavier',
                                                      flag_deformable_convolution=False,
                                                      flag_deformable_convolution_version='v1',
                                                      flag_deformable_convolution_before_or_after_main_convolution='before',
                                                      flag_deformable_convolution_modulation=False,
                                                      flag_deformable_same_on_all_channels=False)

        #### Center RNN: ####
        ### RNN Number of channels: ###
        input_from_upper_level_number_of_channels_Center = self.down2.final_number_of_channels
        number_of_layers_Center = 3;
        hidden_sizes_Center = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Center = hidden_sizes_Center[-1]
        self.hidden_sizes_Center = hidden_sizes_Center
        running_average_input = RGB_input_RunningAverage_number_of_channels * (4 ** 2)
        ### Flags: ###
        self.flag_center_RNN_get_final_RGB_previous = True
        self.flag_center_RNN_get_RGBRunningAverage_current = False
        self.flag_center_RNN_get_RAW_RGB_current = True
        ### Allocate number of channels: ###
        self.center_RNN_number_of_input_channels = self.down2.final_number_of_channels;
        if self.flag_center_RNN_get_final_RGB_previous:
            self.center_RNN_number_of_input_channels += final_RGB_previous_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RAW_RGB_current:
            self.center_RNN_number_of_input_channels += RGB_input_number_of_channels * (4**2)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            self.center_RNN_number_of_input_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
        self.center_RNN = Sequential_Conv_Block_General(
                                                        number_of_input_channels=self.center_RNN_number_of_input_channels,
                                                        number_of_output_channels=[3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor],
                                                        kernel_sizes=[[5], [5], [5]],
                                                        strides=1,
                                                        dilations=[[1], [1], [1]],
                                                        groups=1,
                                                        padding_type='reflect',
                                                        normalization_function=normalization_function,
                                                        activation_function=activation_function,
                                                        mode='CNA',
                                                        initialization_method='xavier',
                                                        flag_dense=False,
                                                        flag_resnet=True,
                                                        flag_concat=False,
                                                        stack_residual_scale=1,
                                                        ##############################################   --  1-K-1 --(*+)---
                                                        ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                                        flag_SuperBlock_SFT=[True, True, True],
                                                        flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                                        flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                                        flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                                        flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                                        flag_SuperBlock_SFT_shift=[True, True, True],
                                                        flag_SuperBlock_SFT_scale=[True, True, True],
                                                        ### Deformable Convolution: ###
                                                        flag_deformable_convolution=[False, False, True],
                                                        flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                                        flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                                        flag_deformable_convolution_modulation=[False, False, False],
                                                        flag_deformable_kernel_size=[5, 5, 5],
                                                        flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                                        flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                                        flag_deformable_same_on_all_channels=[True, True, True],
                                                        ### Deformable SFT: ###
                                                        flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                                        flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                                        flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                                        flag_deformable_SFT_shift=[False, False, False],
                                                        flag_deformable_SFT_scale=[False, False, False],
                                                        flag_deformable_SFT_add_y_to_output=[False, False, False],
                                                        flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                                        #####################################
                                                        flag_single_cell_block_type=['simple', 'simple', 'simple'],
                                                        flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'],
                                                        )

        ### Number of channels further on: ###
        self.number_of_heads_on_top_of_basic_LSTM = 1 #including LSTM_flags output itself
        self.center_output_size = self.number_of_heads_on_top_of_basic_LSTM*hidden_sizes_Center[-1];
        self.number_of_channels_LSTM_CCL_to_decoder = self.center_output_size;
        self.Color_Conversion_Layer_center_to_decoder = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_decoder)
        self.Color_Conversion_Layer_center_to_encoder_next_time = Color_Space_Conversion_Layer(self.center_output_size, self.number_of_channels_LSTM_CCL_to_encoder_next_time)


        ############################################################################################################################################################





        ###########################################################################################################################################################
        #### Decoder2: ####
        ### Number of channels: ###
        input_from_lower_level_number_of_channels_Decoder3 = output_number_of_channels_Center
        number_of_layers_Decoder3 = 2;
        hidden_sizes_Decoder3 = [3 * 4 * 4 * channel_factor, 3 * 4 * 4 * channel_factor  , 3 * 4 * 4 * channel_factor]
        output_number_of_channels_Decoder1 = hidden_sizes_Decoder3[-1]
        ### Flags: ###
        self.flag_up2_use_cross_connection = True
        self.flag_up2_use_encoder_skip_connection = False
        self.flag_up2_get_RGBRunningAverage_current = True
        self.flag_up2_get_RGBRunningAverage_previous = False
        self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_final_RGB_previous = True #TODO: later on add possibility to get it downsampled/unshuffled/only_intensity etc' FOR ALL FLAGS LIKE THIS!!@#
        self.flag_up2_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_center_RNN_previous = False
        self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection = 'cross_connection'
        self.flag_up2_get_RAW_RGB_current = True
        self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up2_lower_level_number_of_channels = self.number_of_channels_LSTM_CCL_to_decoder;
        self.up2_cross_connection_number_of_channels = 0
        # TODO: decider upon a strategy on how to upsample/input every possible input #
        #   (0). Encoder cross connection:
        if self.flag_up2_use_encoder_skip_connection:
            self.up2_cross_connection_number_of_channels += self.down2.final_number_of_channels
        #   (1). RNN previous:
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;
            else:
                self.up2_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder;  #Need to upsample
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                self.up2_lower_level_number_of_channels += RGB_input_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_number_of_channels * (4**1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up2_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**2)
            else:
                self.up2_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
        if self.up2_cross_connection_number_of_channels == 0:
            self.flag_up2_use_cross_connection = False
        ### Define up2: ###
        self.up_concat2 = UNET_Up_General_V1(number_of_lower_level_channels=self.up2_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up2_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up2_cross_connection_number_of_channels,  # Insert shuffled running average input as cross connection... i could also choose to insert it directly from below
                                             number_of_output_channels=hidden_sizes_Decoder3,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=hidden_sizes_Decoder3[-1],
                                             flag_use_cross_connection=self.flag_up2_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             flag_add_unshuffled_input_to_upper_level=False,
                                             activation_function=activation_function,
                                             initialization_method=['xavier', 'xavier'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_SFT_convs_mix=['y', 'y', 'y'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )



        #### Decoder1: ####
        input_from_lower_level_number_of_channels_Decoder2 = self.up_concat2.final_number_of_channels
        number_of_layers_Decoder2 = 2;
        hidden_sizes_Decoder2 = [3 * 4 * channel_factor, 3 * 4 * channel_factor, 3]
        ### Flags: ###
        self.flag_up1_use_cross_connection = True
        self.flag_up1_use_encoder_skip_connection = False
        self.flag_up1_get_center_RNN_previous = False
        self.flag_up1_get_center_RNN_current = True
        self.flag_up1_get_center_RNN_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_center_RGBRunningAverage_previous = False
        self.flag_up1_get_center_RGBRunningAverage_current = False
        self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_final_RGB_previous = True
        self.flag_up1_get_final_RGB_previous_direct_or_cross_connection = 'cross_connection'
        self.flag_up1_get_RAW_RGB_current = True
        self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection = 'cross_connection'
        ### Allocate number of channels: ###
        self.up1_lower_level_number_of_channels = self.up_concat2.final_number_of_channels
        self.up1_cross_connection_number_of_channels = 0
        #   (0). Encoder Cross-Connection:
        if self.flag_up1_use_encoder_skip_connection:
            self.up1_cross_connection_number_of_channels += self.down1.final_number_of_channels;
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
            else:
                self.up1_cross_connection_number_of_channels += self.number_of_channels_LSTM_CCL_to_decoder
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_RunningAverage_number_of_channels * (4**0)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += final_RGB_previous_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += final_RGB_previous_number_of_channels
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                self.up1_lower_level_number_of_channels += RGB_input_number_of_channels * (4**1)
            else:
                self.up1_cross_connection_number_of_channels += RGB_input_number_of_channels
        if self.up1_cross_connection_number_of_channels == 0:
            self.flag_up1_use_cross_connection = False
        ### Define up1: ###
        self.up_concat1 = UNET_Up_General_V1(number_of_lower_level_channels=self.up1_lower_level_number_of_channels,
                                             number_of_lower_level_channels_after_upsample=self.up1_lower_level_number_of_channels,
                                             number_of_cross_connection_channels=self.up1_cross_connection_number_of_channels, #for upper level decoder i insert center output and RA output as "cross-connection"
                                             number_of_output_channels=hidden_sizes_Decoder2,
                                             flag_use_final_projection_block=False,
                                             number_of_output_channels_after_projection_block=3,
                                             flag_use_cross_connection=self.flag_up1_use_cross_connection,
                                             flag_Sequential_or_RDB='sequential',
                                             flag_sequential_dense=False,
                                             flag_sequential_resnet=True,
                                             flag_sequential_concat=False,
                                             stack_residual_scale=1,
                                             kernel_sizes=[[3, 3, 3], [3, 3, 3], [3]], strides=1, dilations=[[1, 2, 4], [1, 2, 4], [1]], groups=1,
                                             normalization_function=normalization_function,
                                             flag_upsample_method='bilinear',
                                             activation_function=final_activation_function,
                                             initialization_method=['dirac', 'dirac', 'dirac'],
                                             ##############################################   --  1-K-1 --(*+)---
                                             ### FIRE/Super Block SFT Self-Interaction: ###   ------------|
                                             flag_SuperBlock_SFT=[True, True, True],
                                             flag_SuperBlock_SFT_use_outside_conditional=[True, True, True],
                                             flag_SuperBlock_SFT_same_on_all_channels=[False, False, False],
                                             flag_SuperBlock_SFT_base_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_SFT_convs_mix=['y', 'y', 'y'],  # 'x', 'y', 'xy'
                                             flag_SuperBlock_SFT_add_y_to_output=[False, False, False],
                                             flag_SuperBlock_SFT_shift=[True, True, True],
                                             flag_SuperBlock_SFT_scale=[True, True, True],
                                             ### Deformable Convolution: ###
                                             flag_deformable_convolution=[False, False, True],
                                             flag_deformable_convolution_version=['v3', 'v3', 'v3'],
                                             flag_deformable_convolution_before_or_after_main_convolution=['before', 'before', 'before'],  # 'before' / 'after'
                                             flag_deformable_convolution_modulation=[False, False, False],
                                             flag_deformable_kernel_size=[5, 5, 5],
                                             flag_deformable_number_of_deformable_groups=[-1, -1, -1],
                                             flag_deformable_number_of_channels_in_group1=[-1, -1, -1],
                                             flag_deformable_same_on_all_channels=[True, True, True],
                                             ### Deformable SFT: ###
                                             flag_deformable_SFT_use_outside_conditional=[False, False, False],
                                             flag_deformable_SFT_same_on_all_channels=[False, False, False],  ### The interactive shift/scale convs by default spits-out x.shape[1] number_of_channels....but maybe to save memory and parameters we want the same interaction on all channels
                                             flag_deformable_SFT_base_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_SFT_convs_mix=['x', 'x', 'x'],
                                             flag_deformable_SFT_shift=[False, False, False],
                                             flag_deformable_SFT_scale=[False, False, False],
                                             flag_deformable_SFT_add_y_to_output=[False, False, False],
                                             flag_deformable_for_each_sub_block_or_for_super_block=['super_block', 'super_block', 'super_block'],  # 'super_block' / 'sub_block'
                                             #####################################
                                             flag_single_cell_block_type=['simple', 'simple', 'simple'],  # 'simple'/ 'standard_residual'/ '131_residual'
                                             flag_super_cell_block_type=['131_collapse', '131_collapse', '131_collapse'], # 'concat' / '131' / '131_collapse' / 'concat_standard_residual' / '131_residual' / '131_collapse_residual'
                                             )

        self.unshuffle1 = UnshufflePixels(2);
        self.unshuffle2 = UnshufflePixels(4);
        self.shuffle1 = ShufflePixels(2);
        self.shuffle2 = ShufflePixels(4);
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)

        self.center_RGB_RunningAverage_output_previous = None
        self.center_RNN_output_to_encoder_previous = None
        self.center_RNN_output_to_decoder_previous = None
        self.final_output_previous = None
        self.final_output_previous_warped = None

        self.AvgPool2d = nn.AvgPool2d(2)
        ##########################################################################################################################################################

    def reset_hidden_states(self):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_hidden_states'):
                current_layer.reset_hidden_states();

    def reset_or_detach_hidden_states(self, reset_flags_list):
        for current_layer in self.children():
            if hasattr(current_layer, 'reset_or_detach_hidden_states'):
                current_layer.reset_or_detach_hidden_states(reset_flags_list);

    def hidden_states_to_device(self, device):
        for current_layer in self.children():
            if hasattr(current_layer, 'hidden_states_to_device'):
                current_layer.hidden_states_to_device(device);

    def forward(self, x, reset_flags_list=[2]):
        ### If i don't have one already then create an instance of TVNet for optical flow of RGB inputs/outputs: ###
        if self.TVNet_layer is None:
            self.TVNet_layer = get_TVNet_instance(x, number_of_iterations=self.TVNet_number_of_iterations, number_of_pyramid_scales=self.TVNet_number_of_pyramid_scales, device=x.device, flag_trainable=False)

        ### Color Space Conversion: ###
        # x = self.Color_Conversion_Layer(x);

        ### Unshuffled Input: ###
        unshuffled_input1 = self.unshuffle1(x)
        unshuffled_input2 = self.unshuffle2(x);


        # ### Previous RNN Outputs: ###
        # self.center_RGB_RunningAverage_output_previous = self.center_RGB_unshuffled_running_average.hidden_states[-1]
        # self.center_RNN_output_previous = self.center.hidden_H[-1]


        #############
        ### UNET: ###
        #############
        ### Preproces (Initialize if needed) previous RNN outputs according to reset_flags_list: ###
        # TODO: generalize this as a function within the RNNs which goes like: create_initial_hidden_state(input_tensor, reset_flags_list)
        if reset_flags_list[0]==1:
            #Zero/Initialize at start of batch:
            self.center_RGB_RunningAverage_output_previous = unshuffled_input2;
            self.center_RNN_output_to_encoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_encoder_next_time, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.center_RNN_output_to_decoder_previous = torch.zeros(x.shape[0], self.number_of_channels_LSTM_CCL_to_decoder, int(x.shape[2]/4), int(x.shape[3]/4)).to(x.device)
            self.final_RGB_output_previous = x; #TODO: understand whether i should use ._copy() or is this enough
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous = x.mean(dim=1, keepdim=True)
            ### Optical Flow Related: ###
            self.final_RGB_output_previous_warped = x;
            if self.flag_input_raw_intensity_instead_of_rgb:
                self.final_RGB_output_previous_warped = x.mean(dim=1, keepdim=True)
            self.delta_x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
            self.delta_y = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3])
        if reset_flags_list[0] == 3:
            self.center_RNN_output_to_encoder_previous = self.center_RNN_output_to_encoder_previous.detach()
            self.center_RNN_output_to_decoder_previous = self.center_RNN_output_to_decoder_previous.detach()
            self.center_RGB_RunningAverage_output_previous = self.center_RGB_RunningAverage_output_previous.detach()
            self.final_RGB_output_previous = self.final_RGB_output_previous.detach()
            # ### Optical Flow Related - USE TVNET!!!!: ###
            # self.delta_x, self.delta_y, x2_warped = self.TVNet_layer(x, self.final_RGB_output_previous)
            # self.delta_x = self.delta_x.detach()
            # self.delta_y = self.delta_y.detach()
            # self.final_RGB_output_previous_warped = x2_warped



        ### Shuffle / Upample Previous RNN Outputs: ##
        #(1). Detach Last Final Output in order to prevent memory overload and because it's probably not that necessary???:
        final_RGB_output_previous = self.final_RGB_output_previous.detach()
        final_RGB_output_previous_unshuffled1 = self.unshuffle1(final_RGB_output_previous)
        final_RGB_output_previous_unshuffled2 = self.unshuffle2(final_RGB_output_previous)
        #(2). Assign meaningful variable names to whole U-Net memories:
        #   (2.1). Raw Input Running Average (probably later on wrapped + some sort of T map (similar to TNR by the way)):
        center_RGB_RunningAverage_output_previous_shuffled1 = self.shuffle1(self.center_RGB_RunningAverage_output_previous)
        center_RGB_RunningAverage_output_previous_shuffled2 = self.shuffle2(self.center_RGB_RunningAverage_output_previous)
        #   (2.2). Center LSTM output derived outputs (1 to encoder & 1 to decoder after two different CCL):
        center_RNN_output_to_encoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_encoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_encoder_previous)
        center_RNN_output_to_decoder_previous_upsampled1 = self.upsample1(self.center_RNN_output_to_decoder_previous)
        center_RNN_output_to_decoder_previous_upsampled2 = self.upsample2(self.center_RNN_output_to_decoder_previous)
        #(3). DownSample Input and Previouse-Output RGB Using AvgPool2d:
        x_intensity = x.mean(dim=1, keepdim=True)

        ### PCD Feature Extraction and Alignment Module: ###
        PCD_output_reference_previous = self.PCD_Extract_and_Align_Module(x_intensity, self.final_RGB_output_previous)  # self.final_RGB_output_previous is also Intensity!
        PCD_output_reference_reference = self.PCD_Extract_and_Align_Module(x_intensity, x_intensity)

        # ### TSA Fusion Module: ###
        # #(1). Combine PCD_reference_previous & PCD_reference_reference to get TSA_Fusion_Module inputs which does Spatio-Temporal Weighing & Mixture of Features:
        # TSA_input = torch.cat([PCD_output_reference_previous.unsqueeze(1),PCD_output_reference_reference.unsqueeze(1)],dim=1)  #[B,C,H,W] -> [B,N,C,H,W]
        # #(2). TSA Module:
        # TSA_output = self.TSA_Fusion_Module(TSA_input)


        #############################################################################
        # ### Conditional Network On Concatenated-Features Input&Output RGB(!): ###
        # y_cell_conditional_input_level1 = torch.cat([x, final_RGB_output_previous],dim=1)
        # y_cell_conditional_input_level2 = torch.cat([unshuffled_input1, final_RGB_output_previous_unshuffled1],dim=1)
        # y_cell_conditional_input_center = torch.cat([unshuffled_input2, final_RGB_output_previous_unshuffled2],dim=1)
        # ### Conditional Network On Each Feature Seperately Input&Output RGB(!): ###
        # x_intensity_downsampled1 = self.AvgPool2d(x_intensity)
        # x_intensity_downsampled2 = self.AvgPool2d(x_intensity_downsampled1)
        # final_RGB_output_previous_downsampled1 = self.AvgPool2d(final_RGB_output_previous)
        # final_RGB_output_previous_downsampled2 = self.AvgPool2d(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_level1_1 = self.Conditionals_Network(x_intensity)
        # y_cell_conditional_input_level2_1 = self.Conditionals_Network(x_intensity_downsampled1)
        # y_cell_conditional_input_center_1 = self.Conditionals_Network(x_intensity_downsampled2)
        # y_cell_conditional_input_level1_2 = self.Conditionals_Network(final_RGB_output_previous)
        # y_cell_conditional_input_level2_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled1)
        # y_cell_conditional_input_center_2 = self.Conditionals_Network(final_RGB_output_previous_downsampled2)
        # y_cell_conditional_input_level1 = torch.cat([y_cell_conditional_input_level1_1, y_cell_conditional_input_level1_2], dim=1)
        # y_cell_conditional_input_level2 = torch.cat([y_cell_conditional_input_level2_1, y_cell_conditional_input_level2_2], dim=1)
        # y_cell_conditional_input_center = torch.cat([y_cell_conditional_input_center_1, y_cell_conditional_input_center_2], dim=1)
        # ### Use TSA Output As Conditionals: ###
        # y_cell_conditional_input_level1 = torch.cat([TSA_output,x],dim=1);
        # y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        # y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        ### Use PCD Outputs As Conditionals: ###
        y_cell_conditional_input_level1 = torch.cat([PCD_output_reference_previous, PCD_output_reference_reference], dim=1);
        y_cell_conditional_input_level1 = self.Conditionals_Network(y_cell_conditional_input_level1)
        y_cell_conditional_input_level2 = self.AvgPool2d(y_cell_conditional_input_level1)
        y_cell_conditional_input_center = self.AvgPool2d(y_cell_conditional_input_level2)
        #############################################################################


        ### Encoder1: ###
        # self.flag_down1_get_final_RGB_previous = True;
        # self.flag_down1_get_center_RGB_RunningAverage_previous = True
        # self.flag_down1_get_center_RNN_previous = True;
        ###
        # encoder1_input = torch.cat([x, final_RGB_output_previous, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)  # regular input = raw_RGB_input + last_final_RGB_output +  RGB_running_average
        # encoder1_outside_connection = torch.cat([center_RNN_output_to_encoder_previous_upsampled2], dim=1);  # outside connection = lstm_output
        encoder1_input = x
        encoder1_outside_connection = None
        if self.flag_down1_get_final_RGB_previous:
            encoder1_input = torch.cat([encoder1_input, final_RGB_output_previous], dim=1)
        if self.flag_down1_get_center_RGB_RunningAverage_previous:
            encoder1_input = torch.cat([encoder1_input, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        if self.flag_down1_get_center_RNN_previous:
            if encoder1_outside_connection is None: encoder1_outside_connection = center_RNN_output_to_encoder_previous_upsampled2
            else:
                encoder1_outside_connection = torch.cat([encoder1_outside_connection, center_RNN_output_to_encoder_previous_upsampled2], dim=1)
        ### Adding PCD output: ###
        encoder1_input = torch.cat([encoder1_input],dim=1)
        encoder1, maxpool1 = self.down1(x=encoder1_input, outside_connection_input=encoder1_outside_connection, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Encoder2: ###
        encoder2_input = maxpool1;
        encoder2_outside_connection = None
        ###
        if self.flag_down2_get_final_RGB_previous:
            encoder2_input = torch.cat([encoder2_input, final_RGB_output_previous_unshuffled1], dim=1)
        if self.flag_down2_get_center_RGB_RunningAverage_previous:
            encoder2_input = torch.cat([encoder2_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        if self.flag_down2_get_RawInputExplicit:
            if encoder2_outside_connection is None: encoder2_outside_connection = unshuffled_input1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, unshuffled_input1], dim=1)
        if self.flag_down2_get_center_RNN_previous:
            if encoder2_outside_connection is None: encoder2_outside_connection = center_RNN_output_to_encoder_previous_upsampled1
            else:
                encoder2_outside_connection = torch.cat([encoder2_outside_connection, center_RNN_output_to_encoder_previous_upsampled1], dim=1)
        encoder2, maxpool2 = self.down2(x=encoder2_input, outside_connection_input=encoder2_outside_connection, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)


        #### Center: ####
        # self.flag_center_RNN_get_final_RGB_previous = True
        # self.flag_center_RNN_get_RGBRunningAverage_current = True
        # self.flag_center_RNN_get_RAW_RGB_current = False
        ###
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        # center_RNN_input = torch.cat([maxpool2, final_RGB_output_previous_unshuffled2, center_RGB_RunningAverage_output], dim=1)
        # center_RNN_output = self.center_RNN(center_RNN_input, reset_flags_list)
        # combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.1). Current RNN Outputs:
        #       (2.1.1). Running Average (RA):
        #           (2.1.1.1). Get Running Average Warped According To Optical Flow.... or maybe simple replace hidden_state with last clean image?!?!?!?
        # TODO: implement!!!
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = warp_tensor(self.center_RGB_unshuffled_running_average.hidden_H[0], self.delta_x, self.delta_y)
        # self.center_RGB_unshuffled_running_average.hidden_H[0] = self.final_RGB_output_previous_warped
        #           (2.1.1.2). Pass Raw RGB Unshuffled Input Into Optical Flow:
        #TODO: make sure that if i don't use it no memory is needed!!!.... i switched it to =unshuffled_input2 for now...
        # center_RGB_RunningAverage_output = self.center_RGB_unshuffled_running_average(unshuffled_input2, reset_flags_list)
        center_RGB_RunningAverage_output = unshuffled_input2;
        #       (2.1.2). LSTM:
        center_RNN_input = maxpool2
        if self.flag_center_RNN_get_final_RGB_previous:
            center_RNN_input = torch.cat([center_RNN_input,final_RGB_output_previous_unshuffled2], dim=1)
        if self.flag_center_RNN_get_RAW_RGB_current:
            center_RNN_input = torch.cat([center_RNN_input, unshuffled_input2], dim=1)
        if self.flag_center_RNN_get_RGBRunningAverage_current:
            center_RNN_input = torch.cat([center_RNN_input, center_RGB_RunningAverage_output], dim=1)
        ### Pass Inputs through center RNN & RunningAverage Layers: ###
        center_RNN_output = self.center_RNN(center_RNN_input, y_cell_conditional=y_cell_conditional_input_center, y_deformable_conditional=None)
        combined_center_RNN_output = torch.cat([center_RNN_output], dim=1)
        #   (2.2). Color-Conversion-Layer for lstm output to reduce large number of channels:
        center_RNN_output_to_decoder_current = self.Color_Conversion_Layer_center_to_decoder(combined_center_RNN_output).detach()
        center_RNN_output_to_encoder_next_time = self.Color_Conversion_Layer_center_to_encoder_next_time(combined_center_RNN_output).detach()
        #   (2.3). Assign previous outputs for NEXT TIME:
        self.center_RGB_RunningAverage_output_previous = center_RGB_RunningAverage_output
        self.center_RNN_output_to_encoder_previous = center_RNN_output_to_encoder_next_time.detach()  #NOTICE!!! - i use detach because i'm not using this output and this is no longer an RNN
        self.center_RNN_output_to_decoder_previous = center_RNN_output_to_decoder_current.detach()
        #   (2.4). Upsample Current RNN Outputs:
        center_RGB_RunningAverage_output_shuffled1 = self.shuffle1(center_RGB_RunningAverage_output)  # running average is full resolution unshuffled for now so i need to shuffle it back (later on i will probably use bilinear upsample or something)
        center_RGB_RunningAverage_output_shuffled2 = self.shuffle2(center_RGB_RunningAverage_output)
        center_RNN_output_to_decoder_upsampled1 = self.upsample1(center_RNN_output_to_decoder_current)
        center_RNN_output_to_decoder_upsampled2 = self.upsample2(center_RNN_output_to_decoder_current)


        ### Decoder2: ###
        decoder2_input = center_RNN_output_to_decoder_current;
        decoder2_cross_connection = None;
        ###
        if self.flag_up2_use_encoder_skip_connection:
            if decoder2_cross_connection is None: decoder2_cross_connection = encoder2
            else:
                decoder2_cross_connection = torch.cat([decoder2_cross_connection, encoder2], dim=1);
        #   (5). RGB RunningAverage current:
        if self.flag_up2_get_RGBRunningAverage_current:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, center_RGB_RunningAverage_output], dim=1)
            else:
                if decoder2_cross_connection is None:
                    decoder2_cross_connection = center_RGB_RunningAverage_output_shuffled1
                else:
                    decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_shuffled1], dim=1)
        #   (1). RNN previous
        if self.flag_up2_get_center_RNN_previous:
            if self.flag_up2_get_center_RNN_prevous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RNN_output_to_decoder_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RNN_output_to_decoder_previous_upsampled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RNN_output_to_decoder_previous_upsampled1], dim=1)
        #   (2). Final RGB previous:
        if self.flag_up2_get_final_RGB_previous:
            if self.flag_up2_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, final_RGB_output_previous_unshuffled2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = final_RGB_output_previous_unshuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, final_RGB_output_previous_unshuffled1], dim=1)
        #   (3). RAW Input RGB current:
        if self.flag_up2_get_RAW_RGB_current:
            if self.flag_up2_get_RAW_RGB_current_direct_or_cross_connection:
                decoder2_input = torch.cat([decoder2_input, unshuffled_input2], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = unshuffled_input1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, unshuffled_input1], dim=1)
        #   (4). RGB RunningAverage previous:
        if self.flag_up2_get_RGBRunningAverage_previous:
            if self.flag_up2_get_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder2_input = torch.cat([decoder2_input, self.center_RGB_RunningAverage_output_previous], dim=1)
            else:
                if decoder2_cross_connection is None:
                     decoder2_cross_connection = center_RGB_RunningAverage_output_previous_shuffled1
                else:
                     decoder2_cross_connection = torch.cat([decoder2_cross_connection, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
        ### Decoder2: ###
        decoder2 = self.up_concat2(input_cross_connection=decoder2_cross_connection, input_low_layer=decoder2_input, y_cell_conditional=y_cell_conditional_input_level2, y_deformable_conditional=None)
                                                                    #(*). NOTE!!!...there is one very important different between lower layer inputs and cross-connection: SPATIAL SIZE....
                                                                                                                            #     the layers from input_low_layer go in with smaller size AND THE LAYER ITSELF CHOOSES HOW TO UPSAMPLE THEM!!!!


        ### Decoder1: ###
        decoder1_input = decoder2;
        decoder1_cross_connection = None
        ### Allocate number of channels: ###
        #   (6). Encoder Cross Connection:
        if self.flag_up2_use_encoder_skip_connection:
            if decoder1_cross_connection is None: decoder1_cross_connection = encoder1
            else:
                decoder1_cross_connection = torch.cat([decoder1_cross_connection, encoder1], dim=1)
        #   (3). RunningAverage current:
        if self.flag_up1_get_center_RGBRunningAverage_current:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_shuffled2], dim=1)
        #   (2). RNN current:
        if self.flag_up1_get_center_RNN_current:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RNN_output_to_decoder_upsampled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_upsampled2], dim=1)
        #   (1). RNN previous:
        if self.flag_up1_get_center_RNN_previous:
            if self.flag_up1_get_center_RNN_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input,center_RNN_output_to_decoder_previous_upsampled1] , dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RNN_output_to_decoder_previous_upsampled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RNN_output_to_decoder_previous_upsampled2], dim=1)
        #   (4). RunningAverage previous:
        if self.flag_up1_get_center_RGBRunningAverage_previous:
            if self.flag_up1_get_center_RGBRunningAverage_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, center_RGB_RunningAverage_output_previous_shuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = center_RGB_RunningAverage_output_previous_shuffled2
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, center_RGB_RunningAverage_output_previous_shuffled2], dim=1)
        #   (5). Final RGB previous:
        if self.flag_up1_get_final_RGB_previous:
            if self.flag_up1_get_final_RGB_previous_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, final_RGB_output_previous_unshuffled1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = final_RGB_output_previous
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, final_RGB_output_previous], dim=1)
        #   (6). RAW RGB current:
        if self.flag_up1_get_RAW_RGB_current:
            if self.flag_up1_get_RAW_RGB_current_direct_or_cross_connection == 'direct':
                decoder1_input = torch.cat([decoder1_input, unshuffled_input1], dim=1)
            else:
                if decoder1_cross_connection is None:
                     decoder1_cross_connection = x
                else:
                     decoder1_cross_connection = torch.cat([decoder1_cross_connection, x], dim=1)
        ### Decoder1: ###
        decoder1 = self.up_concat1(input_cross_connection=decoder1_cross_connection, input_low_layer=decoder1_input, y_cell_conditional=y_cell_conditional_input_level1, y_deformable_conditional=None)


        ### Assign Final Output (Detached for now!!!): ###
        self.final_RGB_output_previous = decoder1.detach()
        if self.flag_input_raw_intensity_instead_of_rgb:
            self.final_RGB_output_previous = self.final_RGB_output_previous.mean(dim=1, keepdim=True)

        ### Add Input Residual If Wanted: ###
        if self.UNET_Residual:
            decoder1 += x;

        return decoder1
#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################














